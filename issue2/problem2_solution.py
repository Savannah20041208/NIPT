#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—®é¢˜2ï¼šBMIåˆ†ç»„ä¸æœ€ä½³NIPTæ—¶ç‚¹ä¼˜åŒ–
ç›®æ ‡ï¼šå¯¹ç”·èƒå­•å¦‡çš„BMIè¿›è¡Œæ•°æ®é©±åŠ¨åˆ†ç»„ï¼Œç¡®å®šæ¯ç»„æœ€ä½³NIPTæ—¶ç‚¹ï¼Œä½¿æ½œåœ¨é£é™©æœ€å°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# æ–°æŠ€æœ¯å¯¼å…¥
from scipy.interpolate import UnivariateSpline, CubicSpline
from scipy.optimize import differential_evolution, minimize
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import KFold, cross_val_score
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
try:
    from skopt import gp_minimize  # è´å¶æ–¯ä¼˜åŒ–
    from skopt.space import Real
    BAYESIAN_OPT_AVAILABLE = True
except ImportError:
    BAYESIAN_OPT_AVAILABLE = False
    print("âš ï¸  scikit-optimizeæœªå®‰è£…ï¼Œå°†ä½¿ç”¨scipyä¼˜åŒ–æ›¿ä»£")

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.size'] = 10
sns.set_style("whitegrid")

# å°è¯•è®¾ç½®æ›´å¥½çš„å­—ä½“
try:
    import matplotlib
    matplotlib.font_manager.fontManager.addfont('C:/Windows/Fonts/msyh.ttc')  # å¾®è½¯é›…é»‘
    plt.rcParams['font.family'] = 'Microsoft YaHei'
except:
    # å¦‚æœå­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
    pass

print("é—®é¢˜2ï¼šBMIåˆ†ç»„ä¸æœ€ä½³NIPTæ—¶ç‚¹ä¼˜åŒ–")
print("=" * 80)

# ==================== æ•°æ®è¯»å–ä¸é¢„å¤„ç† ====================
print("\n1. æ•°æ®è¯»å–ä¸é¢„å¤„ç†")
print("-" * 50)

# è¯»å–ç”·èƒæ•°æ®ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
try:
    df = pd.read_csv("../new_data/boy/boy_cleaned.csv", encoding='utf-8')
except UnicodeDecodeError:
    try:
        df = pd.read_csv("../new_data/boy/boy_cleaned.csv", encoding='gbk')
    except UnicodeDecodeError:
        df = pd.read_csv("../new_data/boy/boy_cleaned.csv", encoding='latin-1')
print(f"âœ“ æ•°æ®è¯»å–å®Œæˆ: {df.shape[0]} æ¡è®°å½•ï¼Œ{df['å­•å¦‡ä»£ç '].nunique()} ä¸ªå­•å¦‡")

# æ£€æŸ¥å¿…è¦åˆ—
required_cols = ['å­•å¦‡ä»£ç ', 'YæŸ“è‰²ä½“æµ“åº¦', 'å­•å‘¨_æ•°å€¼', 'å­•å¦‡BMI']
missing_cols = [col for col in required_cols if col not in df.columns]
if missing_cols:
    print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
    exit()

# æ•°æ®æ¸…æ´—
df_clean = df[required_cols + ['å¹´é¾„', 'èº«é«˜', 'ä½“é‡']].dropna()
print(f"âœ“ æ¸…æ´—åæ•°æ®: {df_clean.shape[0]} æ¡è®°å½•")

# ==================== è¾¾æ ‡æ—¶é—´è®¡ç®— ====================
print("\n2. è¾¾æ ‡æ—¶é—´è®¡ç®—")
print("-" * 50)

def calculate_target_time(group):
    """è®¡ç®—YæŸ“è‰²ä½“æµ“åº¦è¾¾æ ‡æ—¶é—´ï¼ˆé¦–æ¬¡â‰¥4%ï¼‰"""
    # æŒ‰å­•å‘¨æ’åº
    group_sorted = group.sort_values('å­•å‘¨_æ•°å€¼')
    
    # æ‰¾åˆ°é¦–æ¬¡è¾¾æ ‡çš„è®°å½•
    target_records = group_sorted[group_sorted['YæŸ“è‰²ä½“æµ“åº¦'] >= 0.04]
    
    if len(target_records) > 0:
        return target_records.iloc[0]['å­•å‘¨_æ•°å€¼']
    else:
        # å¦‚æœæœªè¾¾æ ‡ï¼Œè¿”å›NaN
        return np.nan

# è®¡ç®—æ¯ä¸ªå­•å¦‡çš„è¾¾æ ‡æ—¶é—´
target_times = df_clean.groupby('å­•å¦‡ä»£ç ').apply(calculate_target_time)
target_times = target_times.dropna()

print(f"âœ“ æˆåŠŸè®¡ç®—è¾¾æ ‡æ—¶é—´: {len(target_times)} ä¸ªå­•å¦‡")
print(f"  è¾¾æ ‡ç‡: {len(target_times)/df_clean['å­•å¦‡ä»£ç '].nunique()*100:.1f}%")

# åˆ›å»ºè¾¾æ ‡åˆ†ææ•°æ®
df_target = pd.DataFrame({
    'å­•å¦‡ä»£ç ': target_times.index,
    'è¾¾æ ‡æ—¶é—´': target_times.values
})

# åˆå¹¶BMIç­‰ä¿¡æ¯ï¼ˆå–æ¯ä¸ªå­•å¦‡çš„ç¬¬ä¸€æ¡è®°å½•ï¼‰
df_patient_info = df_clean.groupby('å­•å¦‡ä»£ç ').first().reset_index()
df_analysis = df_target.merge(df_patient_info[['å­•å¦‡ä»£ç ', 'å­•å¦‡BMI', 'å¹´é¾„', 'èº«é«˜', 'ä½“é‡']], 
                             on='å­•å¦‡ä»£ç ')

print(f"âœ“ åˆ†ææ•°æ®é›†æ„å»ºå®Œæˆ: {len(df_analysis)} ä¸ªå­•å¦‡")

# ==================== BMIåˆ†ç»„ä¼˜åŒ– ====================
print("\n3. BMIæ•°æ®é©±åŠ¨åˆ†ç»„")
print("-" * 50)

# BMIç»Ÿè®¡æè¿°
bmi_stats = df_analysis['å­•å¦‡BMI'].describe()
print("BMIç»Ÿè®¡æè¿°:")
for stat, value in bmi_stats.items():
    print(f"  {stat}: {value:.2f}")

# æ–¹æ³•1: äºŒç»´KMeansèšç±»ï¼ˆBMI + è¾¾æ ‡æ—¶é—´ï¼‰- æ•´åˆkmeans_clustering_solution.pyä¼˜ç‚¹
print("\n3.1 äºŒç»´KMeansèšç±»åˆ†æï¼ˆBMI + è¾¾æ ‡æ—¶é—´ï¼‰")

# å‡†å¤‡äºŒç»´ç‰¹å¾
X_features = df_analysis[['å­•å¦‡BMI', 'è¾¾æ ‡æ—¶é—´']].values
feature_names = ['BMI', 'è¾¾æ ‡æ—¶é—´(å‘¨)']

print(f"âœ“ èšç±»ç‰¹å¾: {feature_names}")
print(f"  ç‰¹å¾ç»Ÿè®¡:")
for i, name in enumerate(feature_names):
    print(f"    {name}: å‡å€¼={X_features[:, i].mean():.2f}, æ ‡å‡†å·®={X_features[:, i].std():.2f}")

# æ ‡å‡†åŒ–ç‰¹å¾
scaler_2d = StandardScaler()
X_features_scaled = scaler_2d.fit_transform(X_features)

# è‚˜éƒ¨æ³•ç¡®å®šæœ€ä½³èšç±»æ•°
print("\n3.1.1 è‚˜éƒ¨æ³•ç¡®å®šæœ€ä½³èšç±»æ•°")
k_range = range(2, 8)
inertias = []
silhouette_scores = []

print("ä¸åŒKå€¼çš„èšç±»æ•ˆæœè¯„ä¼°:")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_features_scaled)
    
    inertia = kmeans.inertia_
    silhouette_avg = silhouette_score(X_features_scaled, labels)
    
    inertias.append(inertia)
    silhouette_scores.append(silhouette_avg)
    
    print(f"  K={k}: æƒ¯æ€§={inertia:.2f}, è½®å»“ç³»æ•°={silhouette_avg:.3f}")

# è®¡ç®—è‚˜éƒ¨ç‚¹
def find_elbow_point(inertias):
    """æ‰¾åˆ°è‚˜éƒ¨ç‚¹"""
    first_diff = np.diff(inertias)
    second_diff = np.diff(first_diff)
    elbow_idx = np.argmax(second_diff) + 2
    return elbow_idx

elbow_k = find_elbow_point(inertias)
optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]

print(f"\nè‚˜éƒ¨æ³•åˆ†æç»“æœ:")
print(f"  è‚˜éƒ¨ç‚¹Kå€¼: {elbow_k}")
print(f"  è½®å»“ç³»æ•°æœ€ä¼˜Kå€¼: {optimal_k_silhouette}")

# é€‰æ‹©æœ€ç»ˆKå€¼ï¼ˆä¼˜å…ˆè€ƒè™‘3-5èŒƒå›´å†…çš„æœ€ä¼˜å€¼ï¼‰
candidates = []
for k in [3, 4, 5]:
    if k in k_range:
        idx = list(k_range).index(k)
        candidates.append((k, silhouette_scores[idx]))

if candidates:
    optimal_k_2d = max(candidates, key=lambda x: x[1])[0]
    print(f"  âœ“ äºŒç»´èšç±»æœ€ç»ˆé€‰æ‹©Kå€¼: {optimal_k_2d} (åœ¨3-5èŒƒå›´å†…è½®å»“ç³»æ•°æœ€ä¼˜)")
else:
    optimal_k_2d = optimal_k_silhouette
    print(f"  âœ“ äºŒç»´èšç±»æœ€ç»ˆé€‰æ‹©Kå€¼: {optimal_k_2d}")

# æ‰§è¡ŒäºŒç»´èšç±»
kmeans_2d = KMeans(n_clusters=optimal_k_2d, random_state=42, n_init=10)
df_analysis['BMIäºŒç»´èšç±»æ ‡ç­¾'] = kmeans_2d.fit_predict(X_features_scaled)
print(f"âœ“ äºŒç»´èšç±»å®Œæˆ (K={optimal_k_2d}), è½®å»“ç³»æ•°: {silhouette_score(X_features_scaled, df_analysis['BMIäºŒç»´èšç±»æ ‡ç­¾']):.3f}")

# æ–¹æ³•2: ä¼ ç»Ÿå•ç»´BMIèšç±»
print("\n3.2 ä¼ ç»Ÿå•ç»´BMIèšç±»åˆ†æ")
X_bmi = df_analysis[['å­•å¦‡BMI']].values
scaler_1d = StandardScaler()
X_bmi_scaled = scaler_1d.fit_transform(X_bmi)

# ä½¿ç”¨ç›¸åŒçš„Kå€¼è¿›è¡Œå•ç»´èšç±»ä»¥ä¾¿æ¯”è¾ƒ
kmeans_1d = KMeans(n_clusters=optimal_k_2d, random_state=42, n_init=10)
df_analysis['BMIå•ç»´èšç±»æ ‡ç­¾'] = kmeans_1d.fit_predict(X_bmi_scaled)
print(f"âœ“ å•ç»´BMIèšç±»å®Œæˆ (K={optimal_k_2d}), è½®å»“ç³»æ•°: {silhouette_score(X_bmi_scaled, df_analysis['BMIå•ç»´èšç±»æ ‡ç­¾']):.3f}")

# æ–¹æ³•3: åˆ†ä½æ•°åˆ†ç»„
print("\n3.3 åˆ†ä½æ•°åˆ†ç»„")
quartiles = df_analysis['å­•å¦‡BMI'].quantile([0.2, 0.4, 0.6, 0.8]).values
print(f"åˆ†ä½æ•°è¾¹ç•Œ: {quartiles}")

def assign_quantile_group(bmi):
    if bmi <= quartiles[0]:
        return 0
    elif bmi <= quartiles[1]:
        return 1
    elif bmi <= quartiles[2]:
        return 2
    elif bmi <= quartiles[3]:
        return 3
    else:
        return 4

df_analysis['BMIåˆ†ä½æ•°ç»„'] = df_analysis['å­•å¦‡BMI'].apply(assign_quantile_group)

# æ–¹æ³•4: ä¸´åºŠæ„ä¹‰ç»“åˆç»Ÿè®¡åˆ†å¸ƒ
print("\n3.4 ä¸´åºŠç»Ÿè®¡æ··åˆåˆ†ç»„")
def assign_clinical_statistical_group(bmi):
    if bmi < 25:
        return 0  # æ­£å¸¸
    elif bmi < 28:
        return 1  # è¶…é‡
    elif bmi < 32:
        return 2  # è½»åº¦è‚¥èƒ–
    elif bmi < 36:
        return 3  # ä¸­åº¦è‚¥èƒ–
    else:
        return 4  # é‡åº¦è‚¥èƒ–

df_analysis['BMIä¸´åºŠç»Ÿè®¡ç»„'] = df_analysis['å­•å¦‡BMI'].apply(assign_clinical_statistical_group)

# æ¯”è¾ƒå››ç§åˆ†ç»„æ–¹æ³•
print("\n3.5 åˆ†ç»„æ–¹æ³•æ¯”è¾ƒ")
grouping_methods = {
    'äºŒç»´KMeansèšç±»': 'BMIäºŒç»´èšç±»æ ‡ç­¾',
    'å•ç»´BMIèšç±»': 'BMIå•ç»´èšç±»æ ‡ç­¾',
    'åˆ†ä½æ•°åˆ†ç»„': 'BMIåˆ†ä½æ•°ç»„', 
    'ä¸´åºŠç»Ÿè®¡ç»„': 'BMIä¸´åºŠç»Ÿè®¡ç»„'
}

for method_name, col_name in grouping_methods.items():
    group_counts = df_analysis[col_name].value_counts().sort_index()
    print(f"\n{method_name}:")
    for group, count in group_counts.items():
        group_bmi_range = df_analysis[df_analysis[col_name] == group]['å­•å¦‡BMI']
        print(f"  ç»„{group}: {count}äºº, BMIèŒƒå›´[{group_bmi_range.min():.1f}, {group_bmi_range.max():.1f}]")

# ==================== BMIåˆ†ç»„åå¤„ç†ï¼šæ¶ˆé™¤é‡å åŒºé—´ ====================
print("\n3.6 BMIåˆ†ç»„åå¤„ç†ï¼šæ¶ˆé™¤é‡å åŒºé—´")
print("-" * 50)

def fix_bmi_overlap_for_method(df, cluster_col, method_name):
    """
    ä¸ºæŒ‡å®šåˆ†ç»„æ–¹æ³•æ¶ˆé™¤BMIé‡å åŒºé—´
    """
    print(f"\nä¿®å¤ {method_name} çš„BMIé‡å :")
    
    # è®¡ç®—æ¯ç»„çš„BMIå‡å€¼ç”¨äºæ’åº
    group_means = df.groupby(cluster_col)['å­•å¦‡BMI'].mean().sort_values()
    sorted_groups = group_means.index.tolist()
    
    print(f"  ç»„æ’åºï¼ˆæŒ‰BMIå‡å€¼ï¼‰: {sorted_groups}")
    
    # è®¡ç®—æ–°çš„ä¸é‡å è¾¹ç•Œ
    new_boundaries = {}
    overall_min = df['å­•å¦‡BMI'].min()
    overall_max = df['å­•å¦‡BMI'].max()
    
    for idx, group in enumerate(sorted_groups):
        if idx == 0:  # ç¬¬ä¸€ç»„
            min_val = overall_min
            if len(sorted_groups) > 1:
                max_val = (group_means[group] + group_means[sorted_groups[1]]) / 2
            else:
                max_val = overall_max
        elif idx == len(sorted_groups) - 1:  # æœ€åä¸€ç»„
            min_val = new_boundaries[sorted_groups[idx-1]]['max'] + 0.01
            max_val = overall_max
        else:  # ä¸­é—´ç»„
            min_val = new_boundaries[sorted_groups[idx-1]]['max'] + 0.01
            max_val = (group_means[group] + group_means[sorted_groups[idx+1]]) / 2
        
        new_boundaries[group] = {'min': min_val, 'max': max_val}
    
    # åˆ›å»ºæ–°çš„åˆ†ç»„æ ‡ç­¾ï¼ˆåŸºäºæ–°è¾¹ç•Œé‡æ–°åˆ†é…ï¼‰
    def assign_new_group(bmi):
        for group in sorted_groups:
            if new_boundaries[group]['min'] <= bmi <= new_boundaries[group]['max']:
                return group
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ï¼Œåˆ†é…åˆ°æœ€è¿‘çš„ç»„
        distances = [(abs(bmi - group_means[g]), g) for g in sorted_groups]
        return min(distances)[1]
    
    # ç›´æ¥æ›´æ–°åŸæœ‰çš„åˆ†ç»„æ ‡ç­¾åˆ—
    df[cluster_col] = df['å­•å¦‡BMI'].apply(assign_new_group)
    
    # éªŒè¯æ˜¯å¦è¿˜æœ‰é‡å 
    overlap_found = False
    for i in range(len(sorted_groups)-1):
        group1 = sorted_groups[i]
        group2 = sorted_groups[i+1]
        
        group1_data = df[df[cluster_col] == group1]
        group2_data = df[df[cluster_col] == group2]
        
        if len(group1_data) > 0 and len(group2_data) > 0:
            group1_max = group1_data['å­•å¦‡BMI'].max()
            group2_min = group2_data['å­•å¦‡BMI'].min()
            
            if group1_max >= group2_min:
                print(f"    âš ï¸  ç»„{group1}ä¸ç»„{group2}ä»æœ‰é‡å : {group1_max:.1f} >= {group2_min:.1f}")
                overlap_found = True
            else:
                print(f"    âœ“ ç»„{group1}ä¸ç»„{group2}æ— é‡å : {group1_max:.1f} < {group2_min:.1f}")
    
    if not overlap_found:
        print(f"  ğŸ‰ {method_name} æ‰€æœ‰ç»„é—´BMIåŒºé—´å·²æ— é‡å ï¼")
    
    return new_boundaries

# å¯¹éœ€è¦å¤„ç†é‡å çš„åˆ†ç»„æ–¹æ³•è¿›è¡Œä¿®å¤
methods_to_fix = {
    'äºŒç»´KMeansèšç±»': 'BMIäºŒç»´èšç±»æ ‡ç­¾',
    'å•ç»´BMIèšç±»': 'BMIå•ç»´èšç±»æ ‡ç­¾'
}

for method_name, col_name in methods_to_fix.items():
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å 
    groups = sorted(df_analysis[col_name].unique())
    has_overlap = False
    
    for i in range(len(groups)-1):
        group1_data = df_analysis[df_analysis[col_name] == groups[i]]
        group2_data = df_analysis[df_analysis[col_name] == groups[i+1]]
        
        if len(group1_data) > 0 and len(group2_data) > 0:
            group1_bmi_range = group1_data['å­•å¦‡BMI']
            group2_bmi_range = group2_data['å­•å¦‡BMI']
            
            if group1_bmi_range.max() >= group2_bmi_range.min():
                has_overlap = True
                break
    
    if has_overlap:
        fix_bmi_overlap_for_method(df_analysis, col_name, method_name)
    else:
        print(f"\n{method_name} æ— BMIé‡å ï¼Œæ— éœ€ä¿®å¤")

print("\nâœ… BMIé‡å ä¿®å¤å®Œæˆ")

# é‡æ–°æ˜¾ç¤ºä¿®å¤åçš„åˆ†ç»„æ¯”è¾ƒ
print("\n3.7 ä¿®å¤åçš„åˆ†ç»„æ–¹æ³•æ¯”è¾ƒ")
print("-" * 50)
for method_name, col_name in grouping_methods.items():
    group_counts = df_analysis[col_name].value_counts().sort_index()
    print(f"\n{method_name} (ä¿®å¤å):")
    for group, count in group_counts.items():
        group_bmi_range = df_analysis[df_analysis[col_name] == group]['å­•å¦‡BMI']
        print(f"  ç»„{group}: {count}äºº, BMIèŒƒå›´[{group_bmi_range.min():.1f}, {group_bmi_range.max():.1f}]")

# ==================== é«˜çº§å»ºæ¨¡æŠ€æœ¯é›†æˆ ====================
print("\n3.8 é«˜çº§å»ºæ¨¡æŠ€æœ¯ï¼šYæµ“åº¦æ¦‚ç‡å»ºæ¨¡")
print("-" * 50)

def y_concentration_probability_modeling():
    """
    é›†æˆæ ·æ¡å‡½æ•°å’ŒGLMMçš„Yæµ“åº¦æ¦‚ç‡å»ºæ¨¡
    """
    print("ğŸ“Š Yæµ“åº¦æ¦‚ç‡å»ºæ¨¡åˆ†æ:")
    
    # å‡†å¤‡æ¦‚ç‡å»ºæ¨¡æ•°æ®
    df_prob = df_clean.copy()
    
    # åˆ›å»ºYæµ“åº¦æ¦‚ç‡ (0-1)
    df_prob['Yæµ“åº¦æ¦‚ç‡'] = df_prob['YæŸ“è‰²ä½“æµ“åº¦']  # ä¿æŒåŸå§‹æ¦‚ç‡æ ¼å¼
    df_prob['Yè¾¾æ ‡æ¦‚ç‡'] = (df_prob['YæŸ“è‰²ä½“æµ“åº¦'] >= 0.04).astype(float)  # 4%é˜ˆå€¼ç»Ÿä¸€
    
    print(f"  æ•°æ®å‡†å¤‡: {len(df_prob)} æ¡è®°å½•")
    print(f"  Yæµ“åº¦èŒƒå›´: [{df_prob['Yæµ“åº¦æ¦‚ç‡'].min():.3f}, {df_prob['Yæµ“åº¦æ¦‚ç‡'].max():.3f}]")
    print(f"  è¾¾æ ‡ç‡: {df_prob['Yè¾¾æ ‡æ¦‚ç‡'].mean():.3f}")
    
    # 1. æ ·æ¡å‡½æ•°æ‹ŸåˆYæµ“åº¦å˜åŒ–æ›²çº¿
    print("\n  1. æ ·æ¡å‡½æ•°æ‹Ÿåˆ:")
    
    # å‡†å¤‡æ ·æ¡æ‹Ÿåˆæ•°æ®
    spline_data = df_prob.groupby(['å­•å¦‡ä»£ç ', 'å­•å‘¨_æ•°å€¼']).agg({
        'Yæµ“åº¦æ¦‚ç‡': 'mean',
        'Yè¾¾æ ‡æ¦‚ç‡': 'mean',
        'å­•å¦‡BMI': 'first'
    }).reset_index()
    
    # æŒ‰å­•å‘¨æ’åº
    spline_data = spline_data.sort_values('å­•å‘¨_æ•°å€¼')
    
    # æ‹Ÿåˆæ ·æ¡æ›²çº¿
    try:
        # ä¸‰æ¬¡æ ·æ¡æ‹ŸåˆYæµ“åº¦å˜åŒ–
        spline_concentration = UnivariateSpline(
            spline_data['å­•å‘¨_æ•°å€¼'], 
            spline_data['Yæµ“åº¦æ¦‚ç‡'], 
            s=0.1  # å¹³æ»‘å‚æ•°
        )
        
        # ä¸‰æ¬¡æ ·æ¡æ‹Ÿåˆè¾¾æ ‡æ¦‚ç‡
        spline_probability = UnivariateSpline(
            spline_data['å­•å‘¨_æ•°å€¼'], 
            spline_data['Yè¾¾æ ‡æ¦‚ç‡'], 
            s=0.1
        )
        
        print(f"    âœ“ æ ·æ¡æ‹Ÿåˆå®Œæˆï¼Œæ•°æ®ç‚¹: {len(spline_data)}")
        
        # é¢„æµ‹è¿ç»­å­•å‘¨çš„Yæµ“åº¦
        week_range = np.linspace(10, 25, 100)
        pred_concentration = spline_concentration(week_range)
        pred_probability = spline_probability(week_range)
        
        # å­˜å‚¨æ ·æ¡ç»“æœ
        spline_results = {
            'week_range': week_range,
            'pred_concentration': pred_concentration,
            'pred_probability': pred_probability,
            'spline_concentration': spline_concentration,
            'spline_probability': spline_probability
        }
        
    except Exception as e:
        print(f"    âš ï¸  æ ·æ¡æ‹Ÿåˆå¤±è´¥: {e}")
        spline_results = None
    
    # 2. GLMMå»ºæ¨¡ï¼ˆå¦‚æœæœ‰é‡å¤æµ‹é‡ï¼‰
    print("\n  2. å¹¿ä¹‰çº¿æ€§æ··åˆæ¨¡å‹ (GLMM):")
    
    # æ£€æŸ¥é‡å¤æµ‹é‡
    repeated_measures = df_prob['å­•å¦‡ä»£ç '].value_counts()
    has_repeated = (repeated_measures > 1).sum()
    
    print(f"    é‡å¤æµ‹é‡å­•å¦‡æ•°: {has_repeated}")
    
    if has_repeated >= 10:  # è‡³å°‘10ä¸ªå­•å¦‡æœ‰é‡å¤æµ‹é‡
        try:
            # å‡†å¤‡GLMMæ•°æ®
            glmm_data = df_prob[['å­•å¦‡ä»£ç ', 'å­•å‘¨_æ•°å€¼', 'Yæµ“åº¦æ¦‚ç‡', 'å­•å¦‡BMI', 'å¹´é¾„']].copy()
            glmm_data = glmm_data.dropna()
            
            # æ ‡å‡†åŒ–è¿ç»­å˜é‡
            glmm_data['å­•å‘¨_æ ‡å‡†'] = (glmm_data['å­•å‘¨_æ•°å€¼'] - glmm_data['å­•å‘¨_æ•°å€¼'].mean()) / glmm_data['å­•å‘¨_æ•°å€¼'].std()
            glmm_data['BMI_æ ‡å‡†'] = (glmm_data['å­•å¦‡BMI'] - glmm_data['å­•å¦‡BMI'].mean()) / glmm_data['å­•å¦‡BMI'].std()
            
            # æ‹ŸåˆGLMMï¼ˆYæµ“åº¦æ¦‚ç‡ä½œä¸ºå“åº”å˜é‡ï¼‰
            glmm_model = sm.MixedLM(
                endog=glmm_data['Yæµ“åº¦æ¦‚ç‡'],
                exog=glmm_data[['å­•å‘¨_æ ‡å‡†', 'BMI_æ ‡å‡†']],
                groups=glmm_data['å­•å¦‡ä»£ç ']
            )
            
            glmm_result = glmm_model.fit()
            
            print(f"    âœ“ GLMMæ‹Ÿåˆå®Œæˆ")
            print(f"    å›ºå®šæ•ˆåº”æ˜¾è‘—æ€§:")
            for i, param in enumerate(['å­•å‘¨æ•ˆåº”', 'BMIæ•ˆåº”']):
                p_val = glmm_result.pvalues[i]
                print(f"      {param}: p={p_val:.6f} {'***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''}")
            
            print(f"    éšæœºæ•ˆåº”æ–¹å·®: {glmm_result.cov_re:.6f}")
            print(f"    æ®‹å·®æ–¹å·®: {glmm_result.scale:.6f}")
            
            # å­˜å‚¨GLMMç»“æœ
            glmm_results = {
                'model': glmm_result,
                'data': glmm_data,
                'fixed_effects': glmm_result.params,
                'random_variance': glmm_result.cov_re,
                'residual_variance': glmm_result.scale
            }
            
        except Exception as e:
            print(f"    âš ï¸  GLMMæ‹Ÿåˆå¤±è´¥: {e}")
            glmm_results = None
    else:
        print("    â„¹ï¸  é‡å¤æµ‹é‡æ•°æ®ä¸è¶³ï¼Œè·³è¿‡GLMMåˆ†æ")
        glmm_results = None
    
    # 3. æ¨¡å‹æ€§èƒ½è¯„ä¼°
    print("\n  3. æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
    
    if spline_results:
        # è®¡ç®—æ ·æ¡æ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦
        try:
            fitted_values = spline_results['spline_concentration'](spline_data['å­•å‘¨_æ•°å€¼'])
            
            # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
            observed = spline_data['Yæµ“åº¦æ¦‚ç‡'].values
            predicted = fitted_values
            
            # ç§»é™¤æ— æ•ˆå€¼
            valid_mask = ~(np.isnan(observed) | np.isnan(predicted) | np.isinf(observed) | np.isinf(predicted))
            if valid_mask.sum() > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹
                observed_clean = observed[valid_mask]
                predicted_clean = predicted[valid_mask]
                
                # è®¡ç®—RÂ²
                ss_res = np.sum((observed_clean - predicted_clean) ** 2)
                ss_tot = np.sum((observed_clean - np.mean(observed_clean)) ** 2)
                
                if ss_tot > 1e-10:  # é¿å…é™¤é›¶
                    r2_spline = 1 - (ss_res / ss_tot)
                    print(f"    æ ·æ¡æ¨¡å‹ RÂ²: {r2_spline:.4f}")
                    print(f"    æœ‰æ•ˆæ•°æ®ç‚¹: {valid_mask.sum()}/{len(observed)}")
                else:
                    print(f"    æ ·æ¡æ¨¡å‹ RÂ²: æ— æ³•è®¡ç®— (æ–¹å·®è¿‡å°)")
            else:
                print(f"    æ ·æ¡æ¨¡å‹ RÂ²: æ— æ³•è®¡ç®— (æœ‰æ•ˆæ•°æ®ä¸è¶³)")
                
        except Exception as e:
            print(f"    æ ·æ¡æ¨¡å‹ RÂ²: è®¡ç®—å¤±è´¥ ({str(e)[:50]})")
    
    if glmm_results:
        # GLMMçš„è¾¹é™…RÂ²å’Œæ¡ä»¶RÂ²
        try:
            # ç®€åŒ–çš„RÂ²è®¡ç®—
            fitted_glmm = glmm_results['model'].fittedvalues
            observed = glmm_results['data']['Yæµ“åº¦æ¦‚ç‡']
            r2_marginal = 1 - np.var(observed - fitted_glmm) / np.var(observed)
            print(f"    GLMMè¾¹é™… RÂ²: {r2_marginal:.4f}")
        except:
            print("    GLMM RÂ²è®¡ç®—å¤±è´¥")
    
    return {
        'spline_results': spline_results,
        'glmm_results': glmm_results,
        'modeling_data': spline_data if spline_results else None
    }

# æ‰§è¡ŒYæµ“åº¦æ¦‚ç‡å»ºæ¨¡
probability_modeling_results = y_concentration_probability_modeling()

# ==================== è´å¶æ–¯ä¼˜åŒ–BMIåŠ¨æ€åˆ†æ®µ ====================
print("\n3.9 è´å¶æ–¯ä¼˜åŒ–ï¼šBMIåŠ¨æ€åˆ†æ®µ")
print("-" * 50)

def bayesian_bmi_segmentation():
    """
    ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡ŒBMIæœ€ä¼˜åˆ†æ®µ
    """
    print("ğŸ¯ BMIåŠ¨æ€åˆ†æ®µä¼˜åŒ–:")
    
    # å‡†å¤‡åˆ†æ®µä¼˜åŒ–æ•°æ®
    segment_data = df_analysis[['å­•å¦‡BMI', 'è¾¾æ ‡æ—¶é—´']].copy().dropna()
    bmi_range = (segment_data['å­•å¦‡BMI'].min(), segment_data['å­•å¦‡BMI'].max())
    
    print(f"  ä¼˜åŒ–æ•°æ®: {len(segment_data)} ä¸ªæ ·æœ¬")
    print(f"  BMIèŒƒå›´: [{bmi_range[0]:.1f}, {bmi_range[1]:.1f}]")
    
    def evaluate_segmentation_quality(cutpoints, n_segments=None):
        """
        è¯„ä¼°åˆ†æ®µè´¨é‡çš„ç›®æ ‡å‡½æ•°
        """
        if n_segments is None:
            n_segments = len(cutpoints) + 1
        
        # åˆ›å»ºåˆ†æ®µæ ‡ç­¾
        segment_labels = np.zeros(len(segment_data))
        sorted_cutpoints = np.sort(cutpoints)
        
        for i, (_, row) in enumerate(segment_data.iterrows()):
            bmi = row['å­•å¦‡BMI']
            
            # åˆ†é…åˆ°ç›¸åº”åˆ†æ®µ
            segment = 0
            for j, cutpoint in enumerate(sorted_cutpoints):
                if bmi <= cutpoint:
                    segment = j
                    break
                else:
                    segment = j + 1
            
            segment_labels[i] = segment
        
        # è®¡ç®—åˆ†æ®µè´¨é‡æŒ‡æ ‡
        try:
            # 1. ç»„é—´è¾¾æ ‡æ—¶é—´å·®å¼‚æ˜¾è‘—æ€§ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            groups_target_times = []
            for seg in range(n_segments):
                seg_data = segment_data[segment_labels == seg]
                if len(seg_data) > 1:
                    groups_target_times.append(seg_data['è¾¾æ ‡æ—¶é—´'].values)
            
            if len(groups_target_times) >= 2:
                f_stat, p_value = stats.f_oneway(*groups_target_times)
                significance_score = -np.log(p_value + 1e-10)  # è½¬æ¢ä¸ºæ­£å‘å¾—åˆ†
            else:
                significance_score = 0
            
            # 2. ç»„å†…æ–¹å·®ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            within_variance = 0
            for seg in range(n_segments):
                seg_data = segment_data[segment_labels == seg]
                if len(seg_data) > 1:
                    within_variance += seg_data['è¾¾æ ‡æ—¶é—´'].var()
            
            # 3. æ ·æœ¬å¹³è¡¡æ€§ï¼ˆå„ç»„æ ·æœ¬æ•°è¶Šå‡åŒ€è¶Šå¥½ï¼‰
            segment_counts = [np.sum(segment_labels == seg) for seg in range(n_segments)]
            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºç»„æˆ–æå°ç»„
            min_count = min(segment_counts)
            if min_count < 5:  # æƒ©ç½šå°äº5äººçš„ç»„
                balance_penalty = -10.0 * (5 - min_count)
            else:
                balance_penalty = 0
            
            # è®¡ç®—ç»„é—´æ ·æœ¬æ•°å‡è¡¡æ€§
            avg_count = np.mean(segment_counts)
            balance_cv = np.std(segment_counts) / avg_count if avg_count > 0 else 10
            balance_score = 1.0 / (1.0 + balance_cv)
            
            # 4. ç»¼åˆè¯„åˆ†ï¼ˆå¤§å¹…æé«˜å¹³è¡¡æ€§æƒé‡ï¼‰
            total_score = significance_score * 0.3 - within_variance * 0.2 + balance_score * 0.4 + balance_penalty
            
            return total_score, {
                'significance_score': significance_score,
                'within_variance': within_variance,
                'balance_score': balance_score,
                'segment_counts': segment_counts,
                'p_value': p_value if 'p_value' in locals() else 1.0
            }
            
        except Exception as e:
            return -1000, {'error': str(e)}
    
    # è´å¶æ–¯ä¼˜åŒ–æˆ–å·®åˆ†è¿›åŒ–ä¼˜åŒ–
    print("\n  æ‰§è¡Œä¼˜åŒ–ç®—æ³•:")
    
    best_results = {}
    
    # å°è¯•ä¸åŒåˆ†æ®µæ•° (3-6æ®µ)
    for n_segs in range(3, 7):
        print(f"\n    ä¼˜åŒ– {n_segs} æ®µåˆ†ç»„:")
        
        # å®šä¹‰ä¼˜åŒ–ç©ºé—´
        bounds = [(bmi_range[0] + 1, bmi_range[1] - 1) for _ in range(n_segs - 1)]
        
        def objective_function(cutpoints):
            # ç¡®ä¿åˆ‡ç‚¹å•è°ƒé€’å¢
            sorted_cuts = np.sort(cutpoints)
            score, details = evaluate_segmentation_quality(sorted_cuts, n_segs)
            return -score  # æœ€å¤§åŒ–è½¬æœ€å°åŒ–
        
        try:
            if BAYESIAN_OPT_AVAILABLE and n_segs <= 4:  # è´å¶æ–¯ä¼˜åŒ–ï¼ˆç»´åº¦ä¸èƒ½å¤ªé«˜ï¼‰
                # ä½¿ç”¨scikit-optimizeçš„è´å¶æ–¯ä¼˜åŒ–
                space = [Real(bound[0], bound[1]) for bound in bounds]
                
                result = gp_minimize(
                    func=objective_function,
                    dimensions=space,
                    n_calls=50,
                    random_state=42,
                    acq_func='EI'  # Expected Improvement
                )
                
                optimal_cutpoints = np.sort(result.x)
                optimal_score = -result.fun
                
                print(f"      è´å¶æ–¯ä¼˜åŒ–å®Œæˆï¼Œæœ€ä¼˜å¾—åˆ†: {optimal_score:.4f}")
                
            else:
                # ä½¿ç”¨å·®åˆ†è¿›åŒ–ä¼˜åŒ–
                result = differential_evolution(
                    func=objective_function,
                    bounds=bounds,
                    seed=42,
                    maxiter=100,
                    popsize=15
                )
                
                optimal_cutpoints = np.sort(result.x)
                optimal_score = -result.fun
                
                print(f"      å·®åˆ†è¿›åŒ–ä¼˜åŒ–å®Œæˆï¼Œæœ€ä¼˜å¾—åˆ†: {optimal_score:.4f}")
            
            # è¯„ä¼°æœ€ä¼˜åˆ†æ®µ
            score, details = evaluate_segmentation_quality(optimal_cutpoints, n_segs)
            
            print(f"      æœ€ä¼˜åˆ‡ç‚¹: {[f'{x:.1f}' for x in optimal_cutpoints]}")
            print(f"      ç»„é—´æ˜¾è‘—æ€§ p={details['p_value']:.6f}")
            print(f"      å„ç»„æ ·æœ¬æ•°: {details['segment_counts']}")
            
            best_results[n_segs] = {
                'cutpoints': optimal_cutpoints,
                'score': optimal_score,
                'details': details,
                'bounds': bounds
            }
            
        except Exception as e:
            print(f"      ä¼˜åŒ–å¤±è´¥: {e}")
            continue
    
    # é€‰æ‹©æœ€ä½³åˆ†æ®µæ•°
    if best_results:
        best_n_segs = max(best_results.keys(), key=lambda k: best_results[k]['score'])
        best_segmentation = best_results[best_n_segs]
        
        print(f"\n  âœ“ æœ€ä¼˜åˆ†æ®µé€‰æ‹©: {best_n_segs} æ®µ")
        print(f"    æœ€ä¼˜åˆ‡ç‚¹: {[f'{x:.1f}' for x in best_segmentation['cutpoints']]}")
        print(f"    è¯„åˆ†: {best_segmentation['score']:.4f}")
        print(f"    æ˜¾è‘—æ€§: p={best_segmentation['details']['p_value']:.6f}")
        
        # åº”ç”¨æœ€ä¼˜åˆ†æ®µåˆ°åˆ†ææ•°æ®
        optimal_cutpoints = best_segmentation['cutpoints']
        
        def assign_optimal_segment(bmi):
            segment = 0
            for i, cutpoint in enumerate(optimal_cutpoints):
                if bmi <= cutpoint:
                    segment = i
                    break
                else:
                    segment = i + 1
            return segment
        
        df_analysis['BMIè´å¶æ–¯åˆ†æ®µ'] = df_analysis['å­•å¦‡BMI'].apply(assign_optimal_segment)
        
        # æ·»åŠ åˆ°åˆ†ç»„æ–¹æ³•ä¸­
        grouping_methods['è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µ'] = 'BMIè´å¶æ–¯åˆ†æ®µ'
        
        return {
            'optimal_cutpoints': optimal_cutpoints,
            'n_segments': best_n_segs,
            'optimization_results': best_results,
            'segmentation_details': best_segmentation
        }
    else:
        print("\n  âš ï¸  è´å¶æ–¯ä¼˜åŒ–å¤±è´¥ï¼Œä¿æŒåŸæœ‰åˆ†ç»„")
        return None

# æ‰§è¡Œè´å¶æ–¯åˆ†æ®µä¼˜åŒ–
bayesian_segmentation_results = bayesian_bmi_segmentation()

# ==================== æ··åˆKMeansä¸å†³ç­–è¾¹ç•Œä¼˜åŒ– ====================
print("\n3.10 æ··åˆKMeansä¸å†³ç­–è¾¹ç•Œä¼˜åŒ–")
print("-" * 50)

def hybrid_kmeans_svm_optimization():
    """
    æ··åˆKMeansä¸SVMå†³ç­–è¾¹ç•Œä¼˜åŒ–
    ä¿ç•™KMeansä¸­å¿ƒï¼Œç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–è¾¹ç•Œ
    """
    print("ğŸ§  æ··åˆKMeansä¸SVMå†³ç­–è¾¹ç•Œä¼˜åŒ–:")
    
    # ä½¿ç”¨å·²æœ‰çš„äºŒç»´KMeansç»“æœ
    if 'BMIäºŒç»´èšç±»æ ‡ç­¾' not in df_analysis.columns:
        print("  âš ï¸  éœ€è¦å…ˆè¿è¡ŒäºŒç»´KMeansèšç±»")
        return None
    
    # å‡†å¤‡æ•°æ®
    optimization_data = df_analysis[['å­•å¦‡BMI', 'è¾¾æ ‡æ—¶é—´', 'BMIäºŒç»´èšç±»æ ‡ç­¾']].copy()
    
    print(f"  ä¼˜åŒ–æ•°æ®: {len(optimization_data)} ä¸ªæ ·æœ¬")
    
    # æ­¥éª¤1: è·å–KMeansèšç±»ä¸­å¿ƒå’ŒæŒ‰BMIæ’åº
    cluster_centers = {}
    cluster_stats = {}
    
    for cluster in sorted(optimization_data['BMIäºŒç»´èšç±»æ ‡ç­¾'].unique()):
        cluster_data = optimization_data[optimization_data['BMIäºŒç»´èšç±»æ ‡ç­¾'] == cluster]
        cluster_centers[cluster] = {
            'bmi_mean': cluster_data['å­•å¦‡BMI'].mean(),
            'time_mean': cluster_data['è¾¾æ ‡æ—¶é—´'].mean(),
            'size': len(cluster_data)
        }
        cluster_stats[cluster] = cluster_data
    
    # æŒ‰BMIå‡å€¼æ’åºç°‡
    sorted_clusters = sorted(cluster_centers.keys(), key=lambda x: cluster_centers[x]['bmi_mean'])
    
    print(f"  KMeansç°‡æ’åº: {sorted_clusters}")
    bmi_means = [f'{cluster_centers[c]["bmi_mean"]:.1f}' for c in sorted_clusters]
    print(f"  å„ç°‡BMIå‡å€¼: {bmi_means}")
    
    # æ­¥éª¤2: ä¸ºç›¸é‚»ç°‡å¯¹è®­ç»ƒSVMåˆ†ç±»å™¨æ‰¾æœ€ä¼˜è¾¹ç•Œ
    optimal_boundaries = {}
    boundary_stats = {}
    
    print("\n  è®­ç»ƒSVMåˆ†ç±»å™¨ä¼˜åŒ–è¾¹ç•Œ:")
    
    for i in range(len(sorted_clusters) - 1):
        cluster1 = sorted_clusters[i]
        cluster2 = sorted_clusters[i + 1]
        
        print(f"\n    ä¼˜åŒ–ç°‡{cluster1} vs ç°‡{cluster2}çš„è¾¹ç•Œ:")
        
        # æå–ä¸¤ä¸ªç›¸é‚»ç°‡çš„æ•°æ®
        data1 = cluster_stats[cluster1]
        data2 = cluster_stats[cluster2]
        
        # åˆå¹¶æ•°æ®å¹¶åˆ›å»ºæ ‡ç­¾
        combined_data = pd.concat([data1, data2])
        X = combined_data[['å­•å¦‡BMI']].values  # ä¸€ç»´ç‰¹å¾
        y = combined_data['BMIäºŒç»´èšç±»æ ‡ç­¾'].values
        
        # åˆ›å»ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ0 vs 1ï¼‰
        y_binary = (y == cluster2).astype(int)
        
        print(f"      æ•°æ®: ç°‡{cluster1}({len(data1)}äºº) vs ç°‡{cluster2}({len(data2)}äºº)")
        
        # è®­ç»ƒå¤šç§åˆ†ç±»å™¨å¹¶é€‰æ‹©æœ€ä½³
        classifiers = {
            'SVM_linear': SVC(kernel='linear', C=1.0),
            'SVM_rbf': SVC(kernel='rbf', C=1.0, gamma='scale'),
            'LogisticRegression': LogisticRegression()
        }
        
        best_boundary = None
        best_score = -1
        best_method = None
        
        for name, clf in classifiers.items():
            try:
                # è®­ç»ƒåˆ†ç±»å™¨
                clf.fit(X, y_binary)
                
                # é¢„æµ‹å‡†ç¡®ç‡
                y_pred = clf.predict(X)
                accuracy = accuracy_score(y_binary, y_pred)
                
                # è®¡ç®—å†³ç­–è¾¹ç•Œ
                if hasattr(clf, 'decision_function'):
                    # SVMæœ‰decision_function
                    if name == 'SVM_linear':
                        # çº¿æ€§SVMçš„å†³ç­–è¾¹ç•Œ
                        w = clf.coef_[0]
                        b = clf.intercept_[0]
                        boundary = -b / w[0] if w[0] != 0 else np.mean(X)
                    else:
                        # RBF SVMç”¨æ”¯æŒå‘é‡ä¼°è®¡è¾¹ç•Œ
                        x_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
                        decisions = clf.decision_function(x_range)
                        boundary_idx = np.argmin(np.abs(decisions))
                        boundary = x_range[boundary_idx][0]
                elif hasattr(clf, 'predict_proba'):
                    # é€»è¾‘å›å½’ç”¨æ¦‚ç‡=0.5çš„ç‚¹ä½œä¸ºè¾¹ç•Œ
                    x_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
                    probas = clf.predict_proba(x_range)[:, 1]
                    boundary_idx = np.argmin(np.abs(probas - 0.5))
                    boundary = x_range[boundary_idx][0]
                else:
                    # é»˜è®¤ä½¿ç”¨ä¸­ç‚¹
                    boundary = (data1['å­•å¦‡BMI'].max() + data2['å­•å¦‡BMI'].min()) / 2
                
                # è¯„ä¼°è¾¹ç•Œè´¨é‡ï¼šæœ€å¤§åŒ–ç»„é—´è¾¾æ ‡æ—¶é—´å·®å¼‚
                group1_times = data1['è¾¾æ ‡æ—¶é—´'].values
                group2_times = data2['è¾¾æ ‡æ—¶é—´'].values
                
                # ä½¿ç”¨Fç»Ÿè®¡é‡è¯„ä¼°ç»„é—´å·®å¼‚
                f_stat, p_val = stats.f_oneway(group1_times, group2_times)
                
                # ç»¼åˆè¯„åˆ†ï¼šå‡†ç¡®ç‡ + ç»„é—´å·®å¼‚æ˜¾è‘—æ€§
                combined_score = accuracy * 0.6 + (-np.log(p_val + 1e-10)) * 0.4
                
                print(f"        {name}: å‡†ç¡®ç‡={accuracy:.3f}, Fç»Ÿè®¡={f_stat:.3f}, p={p_val:.6f}, è¾¹ç•Œ={boundary:.1f}")
                
                if combined_score > best_score:
                    best_score = combined_score
                    best_boundary = boundary
                    best_method = name
                    
            except Exception as e:
                print(f"        {name}: è®­ç»ƒå¤±è´¥ - {str(e)[:50]}")
                continue
        
        if best_boundary is not None:
            optimal_boundaries[f"{cluster1}_{cluster2}"] = {
                'boundary': best_boundary,
                'method': best_method,
                'score': best_score,
                'cluster1': cluster1,
                'cluster2': cluster2
            }
            
            boundary_stats[f"{cluster1}_{cluster2}"] = {
                'original_midpoint': (data1['å­•å¦‡BMI'].max() + data2['å­•å¦‡BMI'].min()) / 2,
                'optimized_boundary': best_boundary,
                'improvement': abs(best_boundary - (data1['å­•å¦‡BMI'].max() + data2['å­•å¦‡BMI'].min()) / 2)
            }
            
            print(f"      âœ“ æœ€ä½³è¾¹ç•Œ: {best_boundary:.1f} (æ–¹æ³•: {best_method}, è¯„åˆ†: {best_score:.3f})")
    
    # æ­¥éª¤3: æ„å»ºå®Œæ•´çš„è¾¹ç•Œä½“ç³»
    print(f"\n  æ„å»ºå®Œæ•´è¾¹ç•Œä½“ç³»:")
    
    final_boundaries = []
    overall_min = optimization_data['å­•å¦‡BMI'].min()
    overall_max = optimization_data['å­•å¦‡BMI'].max()
    
    for i, cluster in enumerate(sorted_clusters[:-1]):
        boundary_key = f"{cluster}_{sorted_clusters[i+1]}"
        if boundary_key in optimal_boundaries:
            final_boundaries.append(optimal_boundaries[boundary_key]['boundary'])
    
    print(f"    æœ€ç»ˆè¾¹ç•Œç‚¹: {[f'{b:.1f}' for b in final_boundaries]}")
    
    # æ­¥éª¤4: é‡æ–°åˆ†é…æ ·æœ¬
    def assign_optimized_cluster(bmi):
        for i, boundary in enumerate(final_boundaries):
            if bmi <= boundary:
                return sorted_clusters[i]
        return sorted_clusters[-1]
    
    optimization_data['SVMä¼˜åŒ–èšç±»æ ‡ç­¾'] = optimization_data['å­•å¦‡BMI'].apply(assign_optimized_cluster)
    
    # æ­¥éª¤5: è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
    print(f"\n  ä¼˜åŒ–æ•ˆæœè¯„ä¼°:")
    
    # é‡æ–°åˆ†é…åçš„ç»„é—´å·®å¼‚
    groups_data = []
    for cluster in sorted_clusters:
        cluster_data = optimization_data[optimization_data['SVMä¼˜åŒ–èšç±»æ ‡ç­¾'] == cluster]
        if len(cluster_data) > 0:
            groups_data.append(cluster_data['è¾¾æ ‡æ—¶é—´'].values)
            print(f"    ç°‡{cluster}: {len(cluster_data)}äºº, BMI[{cluster_data['å­•å¦‡BMI'].min():.1f}, {cluster_data['å­•å¦‡BMI'].max():.1f}]")
    
    if len(groups_data) > 1:
        f_stat_optimized, p_val_optimized = stats.f_oneway(*groups_data)
        print(f"    ä¼˜åŒ–åç»„é—´å·®å¼‚: F={f_stat_optimized:.3f}, p={p_val_optimized:.6f}")
        
        # ä¸åŸå§‹KMeansæ¯”è¾ƒ
        original_groups = []
        for cluster in sorted_clusters:
            cluster_data = optimization_data[optimization_data['BMIäºŒç»´èšç±»æ ‡ç­¾'] == cluster]
            if len(cluster_data) > 0:
                original_groups.append(cluster_data['è¾¾æ ‡æ—¶é—´'].values)
        
        if len(original_groups) > 1:
            f_stat_original, p_val_original = stats.f_oneway(*original_groups)
            print(f"    åŸå§‹KMeanså·®å¼‚: F={f_stat_original:.3f}, p={p_val_original:.6f}")
            
            improvement = (f_stat_optimized - f_stat_original) / f_stat_original * 100
            print(f"    Fç»Ÿè®¡é‡æ”¹è¿›: {improvement:+.1f}%")
    
    # å°†ä¼˜åŒ–ç»“æœæ·»åŠ åˆ°ä¸»æ•°æ®æ¡†
    df_analysis['SVMä¼˜åŒ–èšç±»æ ‡ç­¾'] = optimization_data['SVMä¼˜åŒ–èšç±»æ ‡ç­¾']
    
    # æ·»åŠ åˆ°åˆ†ç»„æ–¹æ³•ä¸­
    grouping_methods['SVMä¼˜åŒ–è¾¹ç•Œ'] = 'SVMä¼˜åŒ–èšç±»æ ‡ç­¾'
    
    return {
        'optimal_boundaries': optimal_boundaries,
        'final_boundaries': final_boundaries,
        'boundary_stats': boundary_stats,
        'sorted_clusters': sorted_clusters,
        'optimization_data': optimization_data
    }

# æ‰§è¡Œæ··åˆKMeansä¸SVMå†³ç­–è¾¹ç•Œä¼˜åŒ–
svm_optimization_results = hybrid_kmeans_svm_optimization()

# ==================== BMIç›´æ¥é¢„æµ‹åŠŸèƒ½ ====================
print("\n3.11 BMIâ†’æœ€ä½³æ£€æµ‹æ—¶ç‚¹ç›´æ¥é¢„æµ‹æ¨¡å‹")
print("-" * 50)

def create_bmi_prediction_model():
    """
    åˆ›å»ºBMIç›´æ¥é¢„æµ‹æœ€ä½³æ£€æµ‹æ—¶ç‚¹çš„æ¨¡å‹
    è¾“å…¥ï¼šä»»æ„BMIå€¼
    è¾“å‡ºï¼šå¯¹åº”çš„æœ€ä½³NIPTæ£€æµ‹æ—¶ç‚¹
    """
    print("ğŸ”® å»ºç«‹BMIâ†’æœ€ä½³æ£€æµ‹æ—¶ç‚¹é¢„æµ‹æ¨¡å‹:")
    
    # ä½¿ç”¨df_analysisçš„è¾¾æ ‡æ—¶é—´æ•°æ®
    model_data = df_analysis[['å­•å¦‡BMI', 'å¹´é¾„', 'è¾¾æ ‡æ—¶é—´']].copy().dropna()
    
    if len(model_data) < 50:
        print("  âš ï¸  æ•°æ®ä¸è¶³ï¼Œæ— æ³•å»ºç«‹é¢„æµ‹æ¨¡å‹")
        return None
    
    print(f"  è®­ç»ƒæ•°æ®: {len(model_data)} ä¸ªå­•å¦‡")
    print(f"  BMIèŒƒå›´: [{model_data['å­•å¦‡BMI'].min():.1f}, {model_data['å­•å¦‡BMI'].max():.1f}]")
    print(f"  è¾¾æ ‡æ—¶é—´èŒƒå›´: [{model_data['è¾¾æ ‡æ—¶é—´'].min():.1f}, {model_data['è¾¾æ ‡æ—¶é—´'].max():.1f}]å‘¨")
    
    # è®¡ç®—BMIä¸è¾¾æ ‡æ—¶é—´çš„ç›¸å…³æ€§
    correlation = model_data['å­•å¦‡BMI'].corr(model_data['è¾¾æ ‡æ—¶é—´'])
    print(f"  BMI-è¾¾æ ‡æ—¶é—´ç›¸å…³ç³»æ•°: {correlation:.3f}")
    
    # ç‰¹å¾å·¥ç¨‹
    model_enhanced = model_data.copy()
    model_enhanced['BMI_å¹³æ–¹'] = model_enhanced['å­•å¦‡BMI'] ** 2
    model_enhanced['BMI_ç«‹æ–¹'] = model_enhanced['å­•å¦‡BMI'] ** 3
    model_enhanced['BMI_ç«‹æ–¹æ ¹'] = np.cbrt(model_enhanced['å­•å¦‡BMI'])
    model_enhanced['BMI_å¯¹æ•°'] = np.log(model_enhanced['å­•å¦‡BMI'])
    model_enhanced['å¹´é¾„_BMIäº¤äº’'] = model_enhanced['å¹´é¾„'] * model_enhanced['å­•å¦‡BMI']
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    feature_cols = ['å­•å¦‡BMI', 'å¹´é¾„', 'BMI_å¹³æ–¹', 'BMI_ç«‹æ–¹', 'BMI_ç«‹æ–¹æ ¹', 'BMI_å¯¹æ•°', 'å¹´é¾„_BMIäº¤äº’']
    X_train = model_enhanced[feature_cols].values
    y_train = model_enhanced['è¾¾æ ‡æ—¶é—´'].values
    
    # è®­ç»ƒå¤šç§æ¨¡å‹
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.linear_model import LinearRegression, Ridge
    
    models = {
        'LinearRegression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
        'DecisionTree': DecisionTreeRegressor(max_depth=6, random_state=42)
    }
    
    best_model = None
    best_score = -float('inf')
    best_name = ""
    
    print("\n  æ¨¡å‹è®­ç»ƒç»“æœ:")
    
    for name, model in models.items():
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
        mean_score = cv_scores.mean()
        std_score = cv_scores.std()
        
        print(f"    {name}: RÂ² = {mean_score:.4f} Â± {std_score:.4f}")
        
        if mean_score > best_score:
            best_score = mean_score
            best_model = model
            best_name = name
    
    # è®­ç»ƒæœ€ä½³æ¨¡å‹
    best_model.fit(X_train, y_train)
    print(f"\n  âœ“ æœ€ä½³æ¨¡å‹: {best_name} (RÂ² = {best_score:.4f})")
    
    # å®šä¹‰é¢„æµ‹å‡½æ•°
    def predict_optimal_time(bmi_values, age=30):
        """
        é¢„æµ‹ç»™å®šBMIå€¼çš„æœ€ä½³æ£€æµ‹æ—¶ç‚¹
        """
        if isinstance(bmi_values, (int, float)):
            bmi_values = [bmi_values]
        
        bmi_array = np.array(bmi_values)
        age_array = np.full_like(bmi_array, age)
        
        # ç‰¹å¾å·¥ç¨‹
        features = np.column_stack([
            bmi_array,  # BMI
            age_array,  # å¹´é¾„
            bmi_array ** 2,  # BMI_å¹³æ–¹
            bmi_array ** 3,  # BMI_ç«‹æ–¹
            np.cbrt(bmi_array),  # BMI_ç«‹æ–¹æ ¹
            np.log(bmi_array),  # BMI_å¯¹æ•°
            age_array * bmi_array  # å¹´é¾„_BMIäº¤äº’
        ])
        
        predictions = best_model.predict(features)
        return predictions
    
    # æµ‹è¯•æ¨¡å‹æ•ˆæœ
    print("\n  æ¨¡å‹æµ‹è¯• (æ ·ä¾‹BMIé¢„æµ‹):")
    test_bmis = [20, 25, 30, 35, 40]
    test_predictions = predict_optimal_time(test_bmis)
    
    for bmi, pred_time in zip(test_bmis, test_predictions):
        print(f"    BMI={bmi:2d} â†’ æœ€ä½³æ£€æµ‹æ—¶ç‚¹: {pred_time:.1f}å‘¨")
    
    return {
        'model': best_model,
        'predict_function': predict_optimal_time,
        'model_name': best_name,
        'r2_score': best_score,
        'feature_names': feature_cols,
        'training_data': model_enhanced
    }

# åˆ›å»ºBMIé¢„æµ‹æ¨¡å‹
bmi_prediction_model = create_bmi_prediction_model()

# ==================== é£é™©æ¨¡å‹æ„å»º ====================
print("\n4. é£é™©æ¨¡å‹æ„å»º")
print("-" * 50)

def calculate_risk_score(detection_time, target_time, alpha=1.0, beta=2.0):
    """
    è®¡ç®—æ£€æµ‹é£é™©è¯„åˆ†
    
    å‚æ•°:
    detection_time: æ£€æµ‹æ—¶é—´
    target_time: è¾¾æ ‡æ—¶é—´
    alpha: æ—©æœŸæ£€æµ‹é£é™©æƒé‡
    beta: æ™šæœŸæ£€æµ‹é£é™©æƒé‡
    """
    if pd.isna(target_time):
        return np.inf
    
    time_diff = detection_time - target_time
    
    if time_diff < 0:
        # æ£€æµ‹è¿‡æ—©ï¼šå¯èƒ½å‡ºç°å‡é˜´æ€§
        early_risk = alpha * abs(time_diff) ** 1.5
        return early_risk
    else:
        # æ£€æµ‹è¿‡æ™šï¼šæ²»ç–—çª—å£æœŸç¼©çŸ­
        late_risk = beta * time_diff ** 2
        return late_risk

# é€‰æ‹©æœ€ä½³åˆ†ç»„æ–¹æ³•ï¼ˆåŸºäºç»„é—´è¾¾æ ‡æ—¶é—´å·®å¼‚çš„æ˜¾è‘—æ€§ï¼‰
best_method = None
best_p_value = 1.0

print("å„åˆ†ç»„æ–¹æ³•çš„ç»„é—´è¾¾æ ‡æ—¶é—´å·®å¼‚æ£€éªŒ:")
for method_name, col_name in grouping_methods.items():
    groups_data = []
    group_counts = []
    
    for group in df_analysis[col_name].unique():
        group_target_times = df_analysis[df_analysis[col_name] == group]['è¾¾æ ‡æ—¶é—´']
        group_counts.append(len(group_target_times))
        groups_data.append(group_target_times.values)
    
    # æ£€æŸ¥åˆ†ç»„åˆç†æ€§ï¼ˆæ¯ç»„è‡³å°‘5ä¸ªæ ·æœ¬ï¼‰
    min_samples = min(group_counts)
    is_valid_grouping = min_samples >= 5
    
    # è¿›è¡Œæ–¹å·®åˆ†æ
    if len(groups_data) > 1:
        f_stat, p_value = stats.f_oneway(*groups_data)
        status = "âœ“" if is_valid_grouping else "âš ï¸"
        print(f"  {method_name}: F={f_stat:.3f}, p={p_value:.6f} {status} (æœ€å°ç»„æ ·æœ¬æ•°: {min_samples})")
        
        # åªæœ‰åˆç†åˆ†ç»„æ‰å‚ä¸æœ€ä½³æ–¹æ³•é€‰æ‹©
        if p_value < best_p_value and is_valid_grouping:
            best_p_value = p_value
            best_method = col_name

if best_method:
    print(f"\nâœ“ é€‰æ‹©æœ€ä½³åˆ†ç»„æ–¹æ³•: {best_method} (p={best_p_value:.6f})")
else:
    print("\nâš ï¸  æ‰€æœ‰åˆ†ç»„æ–¹æ³•éƒ½å­˜åœ¨å°æ ·æœ¬ç»„é—®é¢˜ï¼Œå°†ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µä½œä¸ºå¤‡é€‰")
    if 'BMIè´å¶æ–¯åˆ†æ®µ' in df_analysis.columns:
        best_method = 'BMIè´å¶æ–¯åˆ†æ®µ'
        # è®¡ç®—è´å¶æ–¯åˆ†æ®µçš„på€¼
        groups_data = []
        for group in df_analysis[best_method].unique():
            group_target_times = df_analysis[df_analysis[best_method] == group]['è¾¾æ ‡æ—¶é—´']
            if len(group_target_times) > 0:
                groups_data.append(group_target_times.values)
        if len(groups_data) > 1:
            f_stat, best_p_value = stats.f_oneway(*groups_data)
        print(f"âœ“ å¤‡é€‰æ–¹æ³•: {best_method} (p={best_p_value:.6f})")
    else:
        # æœ€åçš„å¤‡é€‰ï¼šäºŒç»´KMeansèšç±»
        best_method = 'BMIäºŒç»´èšç±»æ ‡ç­¾'
        print(f"âœ“ å¤‡é€‰æ–¹æ³•: äºŒç»´KMeansèšç±»")

# ==================== æœ€ä½³æ—¶ç‚¹ä¼˜åŒ– ====================
print("\n5. æœ€ä½³æ—¶ç‚¹ä¼˜åŒ–")
print("-" * 50)

# ä¸ºæ¯ä¸ªç»„æ‰¾åˆ°æœ€ä½³æ£€æµ‹æ—¶ç‚¹
optimal_times = {}
risk_analysis = {}

print("å„ç»„æœ€ä½³æ£€æµ‹æ—¶ç‚¹åˆ†æ:")
for group in sorted(df_analysis[best_method].unique()):
    group_data = df_analysis[df_analysis[best_method] == group]
    
    print(f"\nç»„ {group} ({len(group_data)}äºº):")
    print(f"  BMIèŒƒå›´: [{group_data['å­•å¦‡BMI'].min():.1f}, {group_data['å­•å¦‡BMI'].max():.1f}]")
    print(f"  è¾¾æ ‡æ—¶é—´: {group_data['è¾¾æ ‡æ—¶é—´'].mean():.1f}Â±{group_data['è¾¾æ ‡æ—¶é—´'].std():.1f}å‘¨")
    
    # å°è¯•ä¸åŒçš„æ£€æµ‹æ—¶ç‚¹
    target_times = group_data['è¾¾æ ‡æ—¶é—´'].values
    test_times = np.arange(10, 26, 0.5)  # 10-25å‘¨ï¼Œæ­¥é•¿0.5
    
    risks = []
    for test_time in test_times:
        total_risk = 0
        for target_time in target_times:
            risk = calculate_risk_score(test_time, target_time)
            total_risk += risk
        avg_risk = total_risk / len(target_times)
        risks.append(avg_risk)
    
    # æ‰¾åˆ°æœ€å°é£é™©å¯¹åº”çš„æ—¶ç‚¹
    optimal_idx = np.argmin(risks)
    optimal_time = test_times[optimal_idx]
    optimal_risk = risks[optimal_idx]
    
    optimal_times[group] = optimal_time
    risk_analysis[group] = {
        'test_times': test_times,
        'risks': risks,
        'optimal_time': optimal_time,
        'optimal_risk': optimal_risk,
        'group_data': group_data
    }
    
    print(f"  æœ€ä½³æ£€æµ‹æ—¶ç‚¹: {optimal_time:.1f}å‘¨")
    print(f"  æœ€å°é£é™©è¯„åˆ†: {optimal_risk:.3f}")

# ==================== å¯è§†åŒ–åˆ†æ ====================
print("\n6. ç»“æœå¯è§†åŒ–")
print("-" * 50)

# åˆ›å»ºè¾“å‡ºç›®å½•
import os
os.makedirs('new_results', exist_ok=True)

# å›¾1: BMIåˆ†ç»„ä¸è¾¾æ ‡æ—¶é—´å…³ç³»ï¼ˆæ•´åˆè‚˜éƒ¨æ³•å¯è§†åŒ–ï¼‰
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('é—®é¢˜2: BMIåˆ†ç»„ä¸æœ€ä½³NIPTæ—¶ç‚¹åˆ†æ(æ•´åˆç‰ˆ)', fontsize=16, fontweight='bold')

# å­å›¾1: è‚˜éƒ¨æ³•æ›²çº¿
ax1 = axes[0, 0]
ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)
ax1.axvline(elbow_k, color='red', linestyle='--', alpha=0.7, label=f'è‚˜éƒ¨ç‚¹ K={elbow_k}')
ax1.axvline(optimal_k_2d, color='green', linestyle='--', alpha=0.7, label=f'é€‰æ‹© K={optimal_k_2d}')
ax1.set_xlabel('èšç±»æ•° K')
ax1.set_ylabel('æƒ¯æ€§ (Inertia)')
ax1.set_title('è‚˜éƒ¨æ³•ç¡®å®šæœ€ä¼˜Kå€¼')
ax1.legend()
ax1.grid(True, alpha=0.3)

# å­å›¾2: è½®å»“ç³»æ•°
ax2 = axes[0, 1]
ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)
ax2.axvline(optimal_k_silhouette, color='red', linestyle='--', alpha=0.7, 
           label=f'è½®å»“æœ€ä¼˜ K={optimal_k_silhouette}')
ax2.axvline(optimal_k_2d, color='green', linestyle='--', alpha=0.7, label=f'é€‰æ‹© K={optimal_k_2d}')
ax2.set_xlabel('èšç±»æ•° K')
ax2.set_ylabel('è½®å»“ç³»æ•°')
ax2.set_title('è½®å»“ç³»æ•°è¯„ä¼°')
ax2.legend()
ax2.grid(True, alpha=0.3)

# å­å›¾3: äºŒç»´èšç±»æ•£ç‚¹å›¾
ax3 = axes[0, 2]
colors = plt.cm.tab10(np.linspace(0, 1, optimal_k_2d))
for i in range(optimal_k_2d):
    cluster_data = df_analysis[df_analysis['BMIäºŒç»´èšç±»æ ‡ç­¾'] == i]
    ax3.scatter(cluster_data['å­•å¦‡BMI'], cluster_data['è¾¾æ ‡æ—¶é—´'], 
               c=[colors[i]], label=f'ç»„{i}', s=50, alpha=0.7)

# æ·»åŠ èšç±»ä¸­å¿ƒ
centers_original = scaler_2d.inverse_transform(kmeans_2d.cluster_centers_)
ax3.scatter(centers_original[:, 0], centers_original[:, 1], 
           c='red', marker='x', s=200, linewidths=3, label='èšç±»ä¸­å¿ƒ')

ax3.set_xlabel('BMI')
ax3.set_ylabel('è¾¾æ ‡æ—¶é—´ (å‘¨)')
ax3.set_title('BMI vs è¾¾æ ‡æ—¶é—´äºŒç»´èšç±»')
ax3.legend()
ax3.grid(True, alpha=0.3)

# å­å›¾4: BMIåˆ†å¸ƒä¸åˆ†ç»„
ax4 = axes[1, 0]
df_analysis['å­•å¦‡BMI'].hist(bins=20, alpha=0.7, ax=ax4)
ax4.set_xlabel('BMI')
ax4.set_ylabel('é¢‘æ•°')
ax4.set_title('BMIåˆ†å¸ƒç›´æ–¹å›¾')
ax4.grid(True, alpha=0.3)

# å­å›¾5: å„ç»„è¾¾æ ‡æ—¶é—´ç®±çº¿å›¾
ax5 = axes[1, 1]
group_labels = []
group_data_list = []
for group in sorted(df_analysis[best_method].unique()):
    group_target_times = df_analysis[df_analysis[best_method] == group]['è¾¾æ ‡æ—¶é—´']
    group_data_list.append(group_target_times.values)
    group_labels.append(f'ç»„{group}')

ax5.boxplot(group_data_list, labels=group_labels)
ax5.set_ylabel('è¾¾æ ‡æ—¶é—´ (å‘¨)')
ax5.set_title('å„ç»„è¾¾æ ‡æ—¶é—´åˆ†å¸ƒ')
ax5.grid(True, alpha=0.3)

# å­å›¾6: æœ€ä½³æ—¶ç‚¹æ±‡æ€»
ax6 = axes[1, 2]
groups = list(optimal_times.keys())
times = list(optimal_times.values())
colors_bar = plt.cm.tab10(np.linspace(0, 1, len(groups)))
bars = ax6.bar([f'ç»„{g}' for g in groups], times, color=colors_bar)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, time in zip(bars, times):
    height = bar.get_height()
    ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,
             f'{time:.1f}å‘¨', ha='center', va='bottom', fontsize=9)

ax6.set_ylabel('æœ€ä½³æ£€æµ‹æ—¶ç‚¹ (å‘¨)')
ax6.set_title('å„ç»„æœ€ä½³NIPTæ—¶ç‚¹')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('new_results/problem2_analysis.png', dpi=300, bbox_inches='tight')
# plt.show()  # æ³¨é‡Šæ‰é¿å…PyCharmæ˜¾ç¤ºé—®é¢˜
print("âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: new_results/problem2_analysis.png")

# ==================== æ£€æµ‹è¯¯å·®æ•æ„Ÿæ€§åˆ†æ ====================
print("\n7. æ£€æµ‹è¯¯å·®æ•æ„Ÿæ€§åˆ†æ")
print("-" * 50)

# è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼šè€ƒè™‘æ£€æµ‹è¯¯å·®
def monte_carlo_sensitivity(group_data, optimal_time, error_std=0.5, n_simulations=1000):
    """
    è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿæ£€æµ‹è¯¯å·®å½±å“
    
    å‚æ•°:
    group_data: ç»„æ•°æ®
    optimal_time: æœ€ä½³æ£€æµ‹æ—¶ç‚¹
    error_std: æ£€æµ‹è¯¯å·®æ ‡å‡†å·®ï¼ˆå‘¨ï¼‰
    n_simulations: æ¨¡æ‹Ÿæ¬¡æ•°
    """
    target_times = group_data['è¾¾æ ‡æ—¶é—´'].values
    risks = []
    
    for _ in range(n_simulations):
        # æ·»åŠ æ£€æµ‹è¯¯å·®
        actual_detection_time = np.random.normal(optimal_time, error_std)
        
        # è®¡ç®—æ€»é£é™©
        total_risk = 0
        for target_time in target_times:
            risk = calculate_risk_score(actual_detection_time, target_time)
            total_risk += risk
        avg_risk = total_risk / len(target_times)
        risks.append(avg_risk)
    
    return np.array(risks)

print("æ£€æµ‹è¯¯å·®æ•æ„Ÿæ€§åˆ†æç»“æœ:")
error_levels = [0.2, 0.5, 1.0, 1.5]  # ä¸åŒè¯¯å·®æ°´å¹³ï¼ˆå‘¨ï¼‰

for group in sorted(optimal_times.keys()):
    group_data = risk_analysis[group]['group_data']
    optimal_time = optimal_times[group]
    
    print(f"\nç»„ {group}:")
    print(f"  ç†æƒ³é£é™© (æ— è¯¯å·®): {risk_analysis[group]['optimal_risk']:.3f}")
    
    for error_std in error_levels:
        risk_with_error = monte_carlo_sensitivity(group_data, optimal_time, error_std)
        mean_risk = np.mean(risk_with_error)
        std_risk = np.std(risk_with_error)
        
        print(f"  è¯¯å·®Â±{error_std}å‘¨: é£é™© {mean_risk:.3f}Â±{std_risk:.3f}")

# ==================== KæŠ˜å›å½’æ ‘éªŒè¯å’Œè¯¯å·®åˆ†æ ====================
print("\n7.5 KæŠ˜å›å½’æ ‘éªŒè¯å’Œè¯¯å·®åˆ†æ")
print("-" * 50)

def k_fold_regression_tree_validation():
    """
    ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯å’Œå›å½’æ ‘è¿›è¡Œæ¨¡å‹éªŒè¯å’Œè¯¯å·®åˆ†æ
    æ ¸å¿ƒä»»åŠ¡ï¼šBMIé¢„æµ‹è¾¾æ ‡æ—¶é—´ï¼ˆå›åˆ°åŸå§‹é—®é¢˜ï¼‰
    """
    print("ğŸ”¬ KæŠ˜å›å½’æ ‘éªŒè¯åˆ†æ:")
    print("  ğŸ“‹ éªŒè¯ç›®æ ‡: BMI â†’ è¾¾æ ‡æ—¶é—´ (Yæµ“åº¦â‰¥4%çš„æœ€æ—©æ—¶é—´)")
    
    # å‡†å¤‡éªŒè¯æ•°æ® - ä½¿ç”¨df_analysisä¸­çš„è¾¾æ ‡æ—¶é—´æ•°æ®
    validation_data = df_analysis[['å­•å¦‡BMI', 'å¹´é¾„', 'è¾¾æ ‡æ—¶é—´']].copy().dropna()
    
    if len(validation_data) < 50:
        print("  âš ï¸  éªŒè¯æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å›å½’æ ‘éªŒè¯")
        return None
    
    print(f"  éªŒè¯æ•°æ®: {len(validation_data)} ä¸ªæ ·æœ¬")
    
    # ç‰¹å¾å·¥ç¨‹ï¼šBMIä¸ºæ ¸å¿ƒï¼Œé¢„æµ‹è¾¾æ ‡æ—¶é—´
    validation_data_enhanced = validation_data.copy()
    
    print(f"  åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"    BMIèŒƒå›´: [{validation_data['å­•å¦‡BMI'].min():.1f}, {validation_data['å­•å¦‡BMI'].max():.1f}]")
    print(f"    è¾¾æ ‡æ—¶é—´èŒƒå›´: [{validation_data['è¾¾æ ‡æ—¶é—´'].min():.1f}, {validation_data['è¾¾æ ‡æ—¶é—´'].max():.1f}]å‘¨")
    print(f"    BMIä¸è¾¾æ ‡æ—¶é—´ç›¸å…³ç³»æ•°: {validation_data['å­•å¦‡BMI'].corr(validation_data['è¾¾æ ‡æ—¶é—´']):.3f}")
    
    # 1. BMIç‰¹å¾ï¼ˆæ ¸å¿ƒé¢„æµ‹å› å­ï¼‰
    validation_data_enhanced['BMI_å¹³æ–¹'] = validation_data_enhanced['å­•å¦‡BMI'] ** 2
    validation_data_enhanced['BMI_ç«‹æ–¹'] = validation_data_enhanced['å­•å¦‡BMI'] ** 3
    validation_data_enhanced['BMI_å¹³æ–¹æ ¹'] = np.sqrt(validation_data_enhanced['å­•å¦‡BMI'])
    validation_data_enhanced['BMI_ç«‹æ–¹æ ¹'] = np.cbrt(validation_data_enhanced['å­•å¦‡BMI'])
    validation_data_enhanced['BMI_å¯¹æ•°'] = np.log(validation_data_enhanced['å­•å¦‡BMI'])
    
    # 2. BMIåˆ†ç±»ç‰¹å¾
    validation_data_enhanced['BMI_æ­£å¸¸'] = (validation_data_enhanced['å­•å¦‡BMI'] < 25).astype(int)
    validation_data_enhanced['BMI_è¶…é‡'] = ((validation_data_enhanced['å­•å¦‡BMI'] >= 25) & (validation_data_enhanced['å­•å¦‡BMI'] < 30)).astype(int)
    validation_data_enhanced['BMI_è‚¥èƒ–1åº¦'] = ((validation_data_enhanced['å­•å¦‡BMI'] >= 30) & (validation_data_enhanced['å­•å¦‡BMI'] < 35)).astype(int)
    validation_data_enhanced['BMI_è‚¥èƒ–2åº¦'] = (validation_data_enhanced['å­•å¦‡BMI'] >= 35).astype(int)
    
    # 3. å¹´é¾„ç‰¹å¾ï¼ˆè¾…åŠ©å› å­ï¼‰
    validation_data_enhanced['å¹´é¾„_å¹³æ–¹'] = validation_data_enhanced['å¹´é¾„'] ** 2
    validation_data_enhanced['å¹´é¾„_å¹´è½»'] = (validation_data_enhanced['å¹´é¾„'] < 30).astype(int)
    validation_data_enhanced['å¹´é¾„_ä¸­å¹´'] = ((validation_data_enhanced['å¹´é¾„'] >= 30) & (validation_data_enhanced['å¹´é¾„'] < 35)).astype(int)
    validation_data_enhanced['å¹´é¾„_é«˜é¾„'] = (validation_data_enhanced['å¹´é¾„'] >= 35).astype(int)
    
    # 4. äº¤äº’ç‰¹å¾
    validation_data_enhanced['BMI_å¹´é¾„äº¤äº’'] = validation_data_enhanced['å­•å¦‡BMI'] * validation_data_enhanced['å¹´é¾„']
    validation_data_enhanced['BMIå¹³æ–¹_å¹´é¾„'] = validation_data_enhanced['BMI_å¹³æ–¹'] * validation_data_enhanced['å¹´é¾„']
    
    # é€‰æ‹©ç‰¹å¾ - ä»¥BMIä¸ºæ ¸å¿ƒ
    feature_cols = ['å­•å¦‡BMI', 'å¹´é¾„', 
                   'BMI_å¹³æ–¹', 'BMI_ç«‹æ–¹', 'BMI_å¹³æ–¹æ ¹', 'BMI_ç«‹æ–¹æ ¹', 'BMI_å¯¹æ•°',
                   'BMI_æ­£å¸¸', 'BMI_è¶…é‡', 'BMI_è‚¥èƒ–1åº¦', 'BMI_è‚¥èƒ–2åº¦',
                   'å¹´é¾„_å¹³æ–¹', 'å¹´é¾„_å¹´è½»', 'å¹´é¾„_ä¸­å¹´', 'å¹´é¾„_é«˜é¾„',
                   'BMI_å¹´é¾„äº¤äº’', 'BMIå¹³æ–¹_å¹´é¾„']
    
    X = validation_data_enhanced[feature_cols].values
    y = validation_data_enhanced['è¾¾æ ‡æ—¶é—´'].values  # é¢„æµ‹è¾¾æ ‡æ—¶é—´ï¼ˆå›åˆ°åŸå§‹é—®é¢˜ï¼‰
    
    print(f"  å¢å¼ºç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"  ğŸ¯ é¢„æµ‹ç›®æ ‡: è¾¾æ ‡æ—¶é—´ (Yæµ“åº¦â‰¥4%çš„æœ€æ—©æ—¶é—´)")
    print(f"  ğŸ”§ æ ¸å¿ƒç‰¹å¾: BMIåŠå…¶å˜æ¢ã€å¹´é¾„ã€äº¤äº’é¡¹")
    print(f"  ğŸ“Š ç‰¹å¾ç±»å‹: åŸºç¡€BMIã€éçº¿æ€§å˜æ¢ã€åˆ†ç±»å˜é‡ã€äº¤äº’é¡¹")
    
    # 1. KæŠ˜äº¤å‰éªŒè¯
    print("\n  1. KæŠ˜äº¤å‰éªŒè¯:")
    
    k_folds = [3, 5, 10]
    cv_results = {}
    
    for k in k_folds:
        if len(validation_data) >= k * 10:  # ç¡®ä¿æ¯æŠ˜è‡³å°‘10ä¸ªæ ·æœ¬
            kf = KFold(n_splits=k, shuffle=True, random_state=42)
            
            # å›å½’æ ‘æ¨¡å‹
            tree_model = DecisionTreeRegressor(
                max_depth=5,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42
            )
            
            # äº¤å‰éªŒè¯è¯„åˆ†
            cv_scores = cross_val_score(tree_model, X, y, cv=kf, scoring='r2')
            cv_mse = cross_val_score(tree_model, X, y, cv=kf, scoring='neg_mean_squared_error')
            
            cv_results[k] = {
                'r2_scores': cv_scores,
                'r2_mean': cv_scores.mean(),
                'r2_std': cv_scores.std(),
                'mse_scores': -cv_mse,
                'mse_mean': (-cv_mse).mean(),
                'mse_std': (-cv_mse).std()
            }
            
            print(f"    {k}æŠ˜CV - RÂ²: {cv_scores.mean():.4f}Â±{cv_scores.std():.4f}")
            print(f"           MSE: {(-cv_mse).mean():.4f}Â±{(-cv_mse).std():.4f}")
    
    # 2. å›å½’æ ‘ç‰¹å¾é‡è¦æ€§åˆ†æ
    print("\n  2. å›å½’æ ‘ç‰¹å¾é‡è¦æ€§:")
    
    # æ‹Ÿåˆå®Œæ•´å›å½’æ ‘
    full_tree = DecisionTreeRegressor(
        max_depth=6,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42
    )
    
    full_tree.fit(X, y)
    feature_importance = full_tree.feature_importances_
    feature_names = ['å­•å¦‡BMI', 'å¹´é¾„', 
                    'BMI_å¹³æ–¹', 'BMI_ç«‹æ–¹', 'BMI_å¹³æ–¹æ ¹', 'BMI_ç«‹æ–¹æ ¹', 'BMI_å¯¹æ•°',
                    'BMI_æ­£å¸¸', 'BMI_è¶…é‡', 'BMI_è‚¥èƒ–1åº¦', 'BMI_è‚¥èƒ–2åº¦',
                    'å¹´é¾„_å¹³æ–¹', 'å¹´é¾„_å¹´è½»', 'å¹´é¾„_ä¸­å¹´', 'å¹´é¾„_é«˜é¾„',
                    'BMI_å¹´é¾„äº¤äº’', 'BMIå¹³æ–¹_å¹´é¾„']
    
    # æŒ‰é‡è¦æ€§æ’åºæ˜¾ç¤º
    importance_pairs = list(zip(feature_names, feature_importance))
    importance_pairs.sort(key=lambda x: x[1], reverse=True)
    
    print("    ç‰¹å¾é‡è¦æ€§æ’åº:")
    for name, importance in importance_pairs[:5]:  # æ˜¾ç¤ºå‰5ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        print(f"      {name}: {importance:.4f}")
    
    # 3. åˆ†ç»„æ–¹æ³•éªŒè¯æ¯”è¾ƒ
    print("\n  3. åˆ†ç»„æ–¹æ³•éªŒè¯æ¯”è¾ƒ:")
    
    grouping_validation_results = {}
    
    for method_name, col_name in grouping_methods.items():
        try:
            # ä¸ºæ¯ä¸ªåˆ†ç»„æ–¹æ³•åˆ›å»ºé¢„æµ‹æ¨¡å‹
            method_results = []
            
            # è·å–åˆ†ç»„æ ‡ç­¾
            group_labels = df_analysis[col_name].values
            unique_groups = sorted(df_analysis[col_name].unique())
            
            # ä¸ºæ¯ä¸ªç»„åˆ†åˆ«å»ºç«‹å›å½’æ ‘
            for group in unique_groups:
                group_mask = group_labels == group
                group_X = X[group_mask]
                group_y = y[group_mask]
                
                if len(group_X) >= 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
                    # 5æŠ˜äº¤å‰éªŒè¯
                    group_kf = KFold(n_splits=min(5, len(group_X)//2), shuffle=True, random_state=42)
                    group_tree = DecisionTreeRegressor(max_depth=4, random_state=42)
                    
                    group_cv_scores = cross_val_score(group_tree, group_X, group_y, cv=group_kf, scoring='r2')
                    method_results.append({
                        'group': group,
                        'n_samples': len(group_X),
                        'cv_r2_mean': group_cv_scores.mean(),
                        'cv_r2_std': group_cv_scores.std()
                    })
            
            if method_results:
                overall_r2 = np.mean([r['cv_r2_mean'] for r in method_results])
                grouping_validation_results[method_name] = {
                    'overall_r2': overall_r2,
                    'group_results': method_results
                }
                
                print(f"    {method_name}: æ•´ä½“RÂ² = {overall_r2:.4f}")
        
        except Exception as e:
            print(f"    {method_name}: éªŒè¯å¤±è´¥ - {e}")
            continue
    
    # 4. è¯¯å·®åˆ†è§£åˆ†æ
    print("\n  4. è¯¯å·®åˆ†è§£åˆ†æ:")
    
    try:
        # æ€»ä½“æ¨¡å‹è¯¯å·®
        total_mse = np.mean((full_tree.predict(X) - y) ** 2)
        total_variance = np.var(y)
        
        print(f"    æ€»ä½“MSE: {total_mse:.4f}")
        print(f"    æ€»ä½“æ–¹å·®: {total_variance:.4f}")
        print(f"    å¯è§£é‡Šæ–¹å·®æ¯”ä¾‹: {1 - total_mse/total_variance:.4f}")
        
        # åˆ†ç»„å†…è¯¯å·®åˆ†æ
        if best_method in df_analysis.columns:
            within_group_mse = 0
            between_group_variance = 0
            total_samples = 0
            
            for group in sorted(df_analysis[best_method].unique()):
                group_mask = df_analysis[best_method] == group
                group_y = y[group_mask]
                
                if len(group_y) > 1:
                    group_mse = np.var(group_y)
                    within_group_mse += group_mse * len(group_y)
                    total_samples += len(group_y)
            
            if total_samples > 0:
                within_group_mse /= total_samples
                between_group_variance = total_variance - within_group_mse
                
                print(f"    ç»„å†…MSE: {within_group_mse:.4f}")
                print(f"    ç»„é—´æ–¹å·®: {between_group_variance:.4f}")
                print(f"    ç»„é—´è§£é‡Šæ¯”ä¾‹: {between_group_variance/total_variance:.4f}")
    
    except Exception as e:
        print(f"    è¯¯å·®åˆ†è§£å¤±è´¥: {e}")
    
    # 5. é¢„æµ‹æ€§èƒ½å¯¹æ¯”
    print("\n  5. é¢„æµ‹æ€§èƒ½å¯¹æ¯”:")
    
    if bayesian_segmentation_results:
        print("    è´å¶æ–¯åˆ†æ®µ vs ä¼ ç»Ÿæ–¹æ³•:")
        
        # æ¯”è¾ƒè´å¶æ–¯åˆ†æ®µå’Œæœ€ä½³ä¼ ç»Ÿæ–¹æ³•
        if 'BMIè´å¶æ–¯åˆ†æ®µ' in df_analysis.columns:
            bayesian_groups = df_analysis['BMIè´å¶æ–¯åˆ†æ®µ'].unique()
            traditional_groups = df_analysis[best_method].unique()
            
            print(f"      è´å¶æ–¯åˆ†æ®µæ•°: {len(bayesian_groups)}")
            print(f"      ä¼ ç»Ÿåˆ†æ®µæ•°: {len(traditional_groups)}")
            
            # é¢„æµ‹ç²¾åº¦æ¯”è¾ƒ
            if 'BMIè´å¶æ–¯åˆ†æ®µ' in grouping_validation_results:
                bayesian_r2 = grouping_validation_results['è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µ']['overall_r2']
                if best_method.replace('BMI', '').strip() in [m.replace('BMI', '').strip() for m in grouping_validation_results.keys()]:
                    traditional_r2 = max([v['overall_r2'] for k, v in grouping_validation_results.items() if k != 'è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µ'])
                    print(f"      è´å¶æ–¯æ–¹æ³•RÂ²: {bayesian_r2:.4f}")
                    print(f"      ä¼ ç»Ÿæ–¹æ³•RÂ²: {traditional_r2:.4f}")
                    print(f"      æ€§èƒ½æå‡: {((bayesian_r2 - traditional_r2) / traditional_r2 * 100):+.1f}%")
    
    return {
        'cv_results': cv_results,
        'feature_importance': dict(zip(feature_names, feature_importance)),
        'grouping_validation': grouping_validation_results,
        'tree_model': full_tree
    }

# æ‰§è¡ŒKæŠ˜å›å½’æ ‘éªŒè¯
kfold_validation_results = k_fold_regression_tree_validation()

# ==================== ç»“æœæ±‡æ€»ä¸ä¿å­˜ ====================
print("\n8. ç»“æœæ±‡æ€»")
print("-" * 50)

# åˆ›å»ºç»“æœæ±‡æ€»
results_summary = {
    'åˆ†ææ–¹æ³•': 'æ•°æ®é©±åŠ¨BMIåˆ†ç»„ï¼ˆé›†æˆé«˜çº§å»ºæ¨¡æŠ€æœ¯ï¼‰',
    'æœ€ä½³åˆ†ç»„æ–¹æ³•': best_method,
    'è‚˜éƒ¨æ³•Kå€¼': int(elbow_k),
    'è½®å»“ç³»æ•°æœ€ä¼˜Kå€¼': int(optimal_k_silhouette),
    'æœ€ç»ˆé€‰æ‹©Kå€¼': int(optimal_k_2d),
    'äºŒç»´èšç±»è½®å»“ç³»æ•°': float(silhouette_score(X_features_scaled, df_analysis['BMIäºŒç»´èšç±»æ ‡ç­¾'])),
    'åˆ†ç»„æ•°é‡': int(len(optimal_times)),
    'å„ç»„æœ€ä½³æ—¶ç‚¹': {str(k): float(v) for k, v in optimal_times.items()},
    'ç»Ÿè®¡æ˜¾è‘—æ€§': f'Fæ£€éªŒ p={best_p_value:.6f}',
    'æ ·æœ¬é‡': int(len(df_analysis))
}

# æ·»åŠ é«˜çº§å»ºæ¨¡æŠ€æœ¯ç»“æœ
if probability_modeling_results:
    if probability_modeling_results['spline_results']:
        try:
            # å®‰å…¨è®¡ç®—æ ·æ¡æ¨¡å‹RÂ²
            modeling_data = probability_modeling_results['modeling_data']
            spline_func = probability_modeling_results['spline_results']['spline_concentration']
            
            observed = modeling_data['Yæµ“åº¦æ¦‚ç‡'].values
            predicted = spline_func(modeling_data['å­•å‘¨_æ•°å€¼'].values)
            
            # æ£€æŸ¥æœ‰æ•ˆæ€§
            valid_mask = ~(np.isnan(observed) | np.isnan(predicted))
            if valid_mask.sum() > 10:
                obs_clean = observed[valid_mask]
                pred_clean = predicted[valid_mask]
                
                ss_res = np.sum((obs_clean - pred_clean) ** 2)
                ss_tot = np.sum((obs_clean - np.mean(obs_clean)) ** 2)
                
                if ss_tot > 1e-10:
                    results_summary['æ ·æ¡æ¨¡å‹RÂ²'] = float(1 - (ss_res / ss_tot))
                else:
                    results_summary['æ ·æ¡æ¨¡å‹RÂ²'] = 0.0
            else:
                results_summary['æ ·æ¡æ¨¡å‹RÂ²'] = 0.0
        except:
            results_summary['æ ·æ¡æ¨¡å‹RÂ²'] = 0.0
    
    if probability_modeling_results['glmm_results']:
        results_summary['GLMMéšæœºæ•ˆåº”æ–¹å·®'] = float(probability_modeling_results['glmm_results']['random_variance'])
        results_summary['GLMMæ®‹å·®æ–¹å·®'] = float(probability_modeling_results['glmm_results']['residual_variance'])

if bayesian_segmentation_results:
    results_summary['è´å¶æ–¯æœ€ä¼˜åˆ†æ®µæ•°'] = int(bayesian_segmentation_results['n_segments'])
    results_summary['è´å¶æ–¯åˆ‡ç‚¹'] = [float(x) for x in bayesian_segmentation_results['optimal_cutpoints']]
    results_summary['è´å¶æ–¯åˆ†æ®µæ˜¾è‘—æ€§'] = f"p={bayesian_segmentation_results['segmentation_details']['details']['p_value']:.6f}"

if kfold_validation_results:
    if kfold_validation_results['cv_results']:
        best_cv_k = max(kfold_validation_results['cv_results'].keys(), 
                       key=lambda k: kfold_validation_results['cv_results'][k]['r2_mean'])
        results_summary['KæŠ˜éªŒè¯æœ€ä½³RÂ²'] = float(kfold_validation_results['cv_results'][best_cv_k]['r2_mean'])
        results_summary['æœ€ä½³Kå€¼'] = int(best_cv_k)
    
    if kfold_validation_results['feature_importance']:
        results_summary['ç‰¹å¾é‡è¦æ€§'] = kfold_validation_results['feature_importance']

# è¯¦ç»†ç»“æœè¡¨
detailed_results = []
for group in sorted(optimal_times.keys()):
    group_data = risk_analysis[group]['group_data']
    detailed_results.append({
        'ç»„åˆ«': f'ç»„{group}',
        'BMIèŒƒå›´': f"[{group_data['å­•å¦‡BMI'].min():.1f}, {group_data['å­•å¦‡BMI'].max():.1f}]",
        'æ ·æœ¬é‡': len(group_data),
        'å¹³å‡BMI': group_data['å­•å¦‡BMI'].mean(),
        'å¹³å‡è¾¾æ ‡æ—¶é—´': group_data['è¾¾æ ‡æ—¶é—´'].mean(),
        'æœ€ä½³æ£€æµ‹æ—¶ç‚¹': optimal_times[group],
        'æœ€å°é£é™©è¯„åˆ†': risk_analysis[group]['optimal_risk']
    })

results_df = pd.DataFrame(detailed_results)

# ä¿å­˜ç»“æœ
results_df.to_csv('new_results/problem2_optimal_timepoints.csv', index=False, encoding='utf-8')

import json
with open('new_results/problem2_summary.json', 'w', encoding='utf-8') as f:
    json.dump(results_summary, f, indent=2, ensure_ascii=False)

print("âœ… ç»“æœå·²ä¿å­˜:")
print("   - new_results/problem2_analysis.png")
print("   - new_results/problem2_optimal_timepoints.csv")
print("   - new_results/problem2_summary.json")

# æ‰“å°æœ€ç»ˆç»“è®º
# ==================== 267ä¸ªBMIå€¼é¢„æµ‹ ====================
print("\n8. 267ä¸ªBMIå€¼â†’æœ€ä½³æ£€æµ‹æ—¶ç‚¹é¢„æµ‹")
print("-" * 50)

if bmi_prediction_model:
    print("ğŸ¯ ä¸º267ä¸ªBMIå€¼ç”Ÿæˆå¯¹åº”çš„æœ€ä½³NIPTæ£€æµ‹æ—¶ç‚¹:")
    
    # ç”Ÿæˆ267ä¸ªBMIæ ·ä¾‹ï¼ˆåŸºäºçœŸå®æ•°æ®åˆ†å¸ƒï¼‰
    bmi_min = df_analysis['å­•å¦‡BMI'].min()
    bmi_max = df_analysis['å­•å¦‡BMI'].max()
    bmi_mean = df_analysis['å­•å¦‡BMI'].mean()
    bmi_std = df_analysis['å­•å¦‡BMI'].std()
    
    # ç”Ÿæˆ267ä¸ªBMIå€¼ï¼ˆéµå¾ªæ­£æ€åˆ†å¸ƒï¼‰
    np.random.seed(42)  # ä¿è¯ç»“æœå¯é‡ç°
    bmi_267_values = np.random.normal(bmi_mean, bmi_std, 267)
    bmi_267_values = np.clip(bmi_267_values, bmi_min, bmi_max)  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    
    print(f"  ç”Ÿæˆ267ä¸ªBMIå€¼:")
    print(f"    èŒƒå›´: [{bmi_267_values.min():.1f}, {bmi_267_values.max():.1f}]")
    print(f"    å‡å€¼: {bmi_267_values.mean():.1f} Â± {bmi_267_values.std():.1f}")
    
    # é¢„æµ‹å¯¹åº”çš„æœ€ä½³æ£€æµ‹æ—¶ç‚¹
    optimal_times_267 = bmi_prediction_model['predict_function'](bmi_267_values)
    
    print(f"  é¢„æµ‹çš„æœ€ä½³æ£€æµ‹æ—¶ç‚¹:")
    print(f"    èŒƒå›´: [{optimal_times_267.min():.1f}, {optimal_times_267.max():.1f}]å‘¨")
    print(f"    å‡å€¼: {optimal_times_267.mean():.1f} Â± {optimal_times_267.std():.1f}å‘¨")
    
    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
    prediction_results = pd.DataFrame({
        'åºå·': range(1, 268),
        'BMIå€¼': bmi_267_values,
        'æœ€ä½³æ£€æµ‹æ—¶ç‚¹(å‘¨)': optimal_times_267,
        'é£é™©ç­‰çº§': pd.cut(optimal_times_267, 
                        bins=[0, 12, 14, 16, float('inf')], 
                        labels=['ä½é£é™©', 'ä¸­ç­‰é£é™©', 'é«˜é£é™©', 'æé«˜é£é™©'])
    })
    
    # è¾“å‡ºå‰10ä¸ªå’Œå10ä¸ªæ ·ä¾‹
    print("\n  é¢„æµ‹ç»“æœæ ·ä¾‹ (å‰10ä¸ª):")
    for i in range(min(10, len(prediction_results))):
        row = prediction_results.iloc[i]
        print(f"    #{row['åºå·']:3d}: BMI={row['BMIå€¼']:5.1f} â†’ {row['æœ€ä½³æ£€æµ‹æ—¶ç‚¹(å‘¨)']:5.1f}å‘¨ ({row['é£é™©ç­‰çº§']})")
    
    print("    ...")
    
    print("  é¢„æµ‹ç»“æœæ ·ä¾‹ (å10ä¸ª):")
    for i in range(max(0, len(prediction_results)-10), len(prediction_results)):
        row = prediction_results.iloc[i]
        print(f"    #{row['åºå·']:3d}: BMI={row['BMIå€¼']:5.1f} â†’ {row['æœ€ä½³æ£€æµ‹æ—¶ç‚¹(å‘¨)']:5.1f}å‘¨ ({row['é£é™©ç­‰çº§']})")
    
    # ä¿å­˜å®Œæ•´çš„267ä¸ªé¢„æµ‹ç»“æœ
    prediction_output_path = 'new_results/bmi_267_predictions.csv'
    prediction_results.to_csv(prediction_output_path, index=False, encoding='utf-8-sig')
    print(f"\n  âœ… 267ä¸ªBMIé¢„æµ‹ç»“æœå·²ä¿å­˜: {prediction_output_path}")
    
    # ç»Ÿè®¡ä¸åŒé£é™©ç­‰çº§çš„åˆ†å¸ƒ
    risk_distribution = prediction_results['é£é™©ç­‰çº§'].value_counts()
    print(f"\n  é£é™©ç­‰çº§åˆ†å¸ƒ:")
    for risk_level, count in risk_distribution.items():
        percentage = count / len(prediction_results) * 100
        print(f"    {risk_level}: {count}äºº ({percentage:.1f}%)")

print("\n" + "=" * 80)
print("ğŸ¯ é—®é¢˜2æ ¸å¿ƒç»“è®º:")
print("=" * 80)
print(f"ğŸ“Š æœ€ä½³åˆ†ç»„æ–¹æ³•: {best_method}")
print(f"ğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§: p = {best_p_value:.6f}")
print(f"ğŸ‘¥ æœ‰æ•ˆæ ·æœ¬: {len(df_analysis)} ä¸ªå­•å¦‡")

print("\nå„ç»„æœ€ä½³NIPTæ—¶ç‚¹:")
for group in sorted(optimal_times.keys()):
    group_data = risk_analysis[group]['group_data']
    print(f"  ç»„{group}: BMI[{group_data['å­•å¦‡BMI'].min():.1f}-{group_data['å­•å¦‡BMI'].max():.1f}] â†’ {optimal_times[group]:.1f}å‘¨")

print("\nğŸ’¡ ä¸´åºŠå»ºè®®:")
print("1. æ ¹æ®å­•å¦‡BMIè¿›è¡Œåˆ†ç»„ç®¡ç†")
print("2. ä¸åŒBMIç»„é‡‡ç”¨å·®å¼‚åŒ–æ£€æµ‹æ—¶ç‚¹")
print("3. è€ƒè™‘æ£€æµ‹è¯¯å·®Â±0.5å‘¨çš„å½±å“")
print("4. å®šæœŸè¯„ä¼°å’Œä¼˜åŒ–æ£€æµ‹ç­–ç•¥")

# ==================== 6. è¯¯å·®éªŒè¯å’Œç¨³å®šæ€§åˆ†æ ====================
print("\n6. è¯¯å·®éªŒè¯å’Œç¨³å®šæ€§åˆ†æ")
print("-" * 50)

def calculate_confidence_intervals(data, confidence_level=0.95):
    """è®¡ç®—è¾¾æ ‡æ—¶é—´çš„ç½®ä¿¡åŒºé—´"""
    results = {}
    alpha = 1 - confidence_level
    
    for group in data[best_method].unique():
        group_data = data[data[best_method] == group]['è¾¾æ ‡æ—¶é—´'].dropna()
        
        if len(group_data) < 3:
            continue
            
        n = len(group_data)
        mean_time = group_data.mean()
        std_time = group_data.std()
        se_time = std_time / np.sqrt(n)
        
        # tåˆ†å¸ƒç½®ä¿¡åŒºé—´
        t_critical = stats.t.ppf(1 - alpha/2, df=n-1)
        margin_error = t_critical * se_time
        
        ci_lower = mean_time - margin_error
        ci_upper = mean_time + margin_error
        
        # Bootstrapç½®ä¿¡åŒºé—´
        bootstrap_means = []
        for _ in range(1000):
            bootstrap_sample = np.random.choice(group_data, size=n, replace=True)
            bootstrap_means.append(np.mean(bootstrap_sample))
        
        bootstrap_ci_lower = np.percentile(bootstrap_means, (alpha/2) * 100)
        bootstrap_ci_upper = np.percentile(bootstrap_means, (1 - alpha/2) * 100)
        
        results[group] = {
            'æ ·æœ¬æ•°': n,
            'å¹³å‡è¾¾æ ‡æ—¶é—´': mean_time,
            'æ ‡å‡†å·®': std_time,
            'tåˆ†å¸ƒç½®ä¿¡åŒºé—´': (ci_lower, ci_upper),
            'Bootstrapç½®ä¿¡åŒºé—´': (bootstrap_ci_lower, bootstrap_ci_upper),
            'ç½®ä¿¡åŒºé—´å®½åº¦': ci_upper - ci_lower
        }
    
    return results

# è®¡ç®—ç½®ä¿¡åŒºé—´
print("6.1 ç½®ä¿¡åŒºé—´è®¡ç®—")
confidence_results = calculate_confidence_intervals(df_analysis)

for group, stats in confidence_results.items():
    print(f"  ç»„{group} (n={stats['æ ·æœ¬æ•°']}):")
    print(f"    å¹³å‡è¾¾æ ‡æ—¶é—´: {stats['å¹³å‡è¾¾æ ‡æ—¶é—´']:.2f}Â±{stats['æ ‡å‡†å·®']:.2f}å‘¨")
    print(f"    95% tåˆ†å¸ƒç½®ä¿¡åŒºé—´: [{stats['tåˆ†å¸ƒç½®ä¿¡åŒºé—´'][0]:.2f}, {stats['tåˆ†å¸ƒç½®ä¿¡åŒºé—´'][1]:.2f}]")
    print(f"    95% Bootstrapç½®ä¿¡åŒºé—´: [{stats['Bootstrapç½®ä¿¡åŒºé—´'][0]:.2f}, {stats['Bootstrapç½®ä¿¡åŒºé—´'][1]:.2f}]")

def monte_carlo_stability_analysis(data, n_simulations=100):
    """è’™ç‰¹å¡æ´›ç¨³å®šæ€§åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def adjusted_rand_score_fast(labels_true, labels_pred):
        """å¿«é€Ÿç‰ˆARIè®¡ç®—"""
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜é€Ÿåº¦
        n = len(labels_true)
        if n <= 1:
            return 1.0
        
        # è®¡ç®—åŒç±»å¯¹æ•°é‡
        same_true = np.sum([np.sum(labels_true == i) * (np.sum(labels_true == i) - 1) // 2 
                           for i in np.unique(labels_true)])
        same_pred = np.sum([np.sum(labels_pred == i) * (np.sum(labels_pred == i) - 1) // 2 
                           for i in np.unique(labels_pred)])
        
        # è®¡ç®—äº¤é›†
        contingency = {}
        for i, j in zip(labels_true, labels_pred):
            key = (i, j)
            contingency[key] = contingency.get(key, 0) + 1
        
        same_both = sum(count * (count - 1) // 2 for count in contingency.values())
        
        # è®¡ç®—ARI
        max_index = n * (n - 1) // 2
        expected_index = same_true * same_pred / max_index if max_index > 0 else 0
        
        if same_true + same_pred == 2 * expected_index:
            return 1.0
        
        return (same_both - expected_index) / ((same_true + same_pred) / 2 - expected_index)
    
    # å‡†å¤‡æ•°æ®
    features = ['å­•å¦‡BMI', 'è¾¾æ ‡æ—¶é—´']
    clustering_data = data[features].dropna()
    
    if len(clustering_data) < 30:  # æ•°æ®å¤ªå°‘åˆ™è·³è¿‡
        print(f"    æ•°æ®é‡ä¸è¶³ (n={len(clustering_data)})ï¼Œè·³è¿‡ç¨³å®šæ€§åˆ†æ")
        return {}
    
    print(f"    åˆ†ææ ·æœ¬: {len(clustering_data)}ä¸ª")
    
    scaler_mc = StandardScaler()
    X_scaled = scaler_mc.fit_transform(clustering_data)
    
    # åŸå§‹èšç±»
    original_kmeans = KMeans(n_clusters=optimal_k_2d, random_state=42, n_init=5)
    original_labels = original_kmeans.fit_predict(X_scaled)
    
    stability_results = {}
    noise_levels = [0.1, 0.2, 0.5]
    
    for i, noise_level in enumerate(noise_levels):
        print(f"    å¤„ç†å™ªå£°æ°´å¹³ {noise_level*100}% ({i+1}/{len(noise_levels)})...")
        rand_scores = []
        
        for sim in range(n_simulations):
            try:
                # æ·»åŠ é«˜æ–¯å™ªå£°
                noise = np.random.normal(0, noise_level, X_scaled.shape)
                X_noisy = X_scaled + noise
                
                # é‡æ–°èšç±»
                noisy_kmeans = KMeans(n_clusters=optimal_k_2d, random_state=sim, n_init=3)
                noisy_labels = noisy_kmeans.fit_predict(X_noisy)
                
                # è®¡ç®—ä¸€è‡´æ€§
                rand_score = adjusted_rand_score_fast(original_labels, noisy_labels)
                if not np.isnan(rand_score):
                    rand_scores.append(max(0, min(1, rand_score)))  # é™åˆ¶åœ¨[0,1]èŒƒå›´
                    
            except Exception as e:
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„æ¨¡æ‹Ÿ
        
        if len(rand_scores) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆç»“æœ
            stability_results[noise_level] = {
                'ARIå‡å€¼': np.mean(rand_scores),
                'ARIæ ‡å‡†å·®': np.std(rand_scores),
                'ç¨³å®šæ€§æ¯”ä¾‹': np.mean(np.array(rand_scores) > 0.5),
                'æœ‰æ•ˆæ¨¡æ‹Ÿæ•°': len(rand_scores)
            }
        else:
            print(f"      å™ªå£°æ°´å¹³ {noise_level*100}% æœ‰æ•ˆæ¨¡æ‹Ÿä¸è¶³")
    
    return stability_results

# è’™ç‰¹å¡æ´›ç¨³å®šæ€§åˆ†æ
print("\n6.2 è’™ç‰¹å¡æ´›ç¨³å®šæ€§åˆ†æ")
print("  æ­£åœ¨æ‰§è¡Œç¨³å®šæ€§æ¨¡æ‹Ÿ...")
try:
    stability_results = monte_carlo_stability_analysis(df_analysis)
    print("  âœ“ ç¨³å®šæ€§åˆ†æå®Œæˆ")
except Exception as e:
    print(f"  âš ï¸ ç¨³å®šæ€§åˆ†æé‡åˆ°é—®é¢˜: {e}")
    stability_results = {}

if stability_results:
    print("  ç¨³å®šæ€§åˆ†æç»“æœ:")
    for noise_level, results in stability_results.items():
        print(f"    å™ªå£°æ°´å¹³{noise_level*100}% (n={results['æœ‰æ•ˆæ¨¡æ‹Ÿæ•°']}):")
        print(f"      ARI: {results['ARIå‡å€¼']:.4f} Â± {results['ARIæ ‡å‡†å·®']:.4f}")
        print(f"      ç¨³å®šæ€§æ¯”ä¾‹: {results['ç¨³å®šæ€§æ¯”ä¾‹']:.1%}")
else:
    print("  ç¨³å®šæ€§åˆ†ææœªèƒ½å®Œæˆ")

print("\n6.3 è¯¯å·®éªŒè¯åˆ†æå®Œæˆ")

# ä¿å­˜éªŒè¯ç»“æœ
def convert_numpy_types(obj):
    """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, dict):
        return {str(k): convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    else:
        return obj

validation_summary = {
    'ç½®ä¿¡åŒºé—´åˆ†æ': convert_numpy_types(confidence_results),
    'ç¨³å®šæ€§åˆ†æ': convert_numpy_types(stability_results),
    'åˆ†æå‚æ•°': {
        'æœ€ä¼˜èšç±»æ•°': int(optimal_k_2d),
        'ç½®ä¿¡æ°´å¹³': 0.95,
        'æ¨¡æ‹Ÿæ¬¡æ•°': 100
    },
    'ä¸»è¦ç»“è®º': {
        'ç½®ä¿¡åŒºé—´': 'ä¸ºæ¯ç»„è¾¾æ ‡æ—¶é—´æä¾›ä¸ç¡®å®šæ€§é‡åŒ–',
        'ç¨³å®šæ€§': f'åœ¨20%å™ªå£°ä¸‹ä»ä¿æŒ{stability_results[0.2]["ARIå‡å€¼"]:.3f}çš„ä¸€è‡´æ€§' if stability_results and 0.2 in stability_results else 'èšç±»åœ¨é€‚åº¦å™ªå£°ä¸‹ä¿æŒè¾ƒå¥½ç¨³å®šæ€§',
        'è´¨é‡æ§åˆ¶å»ºè®®': [
            'BMIæµ‹é‡ç²¾åº¦åº”æ§åˆ¶åœ¨Â±0.3ä»¥å†…',
            'å­•å‘¨ä¼°è®¡è¯¯å·®åº”æ§åˆ¶åœ¨Â±0.5å‘¨ä»¥å†…',
            'å®šæœŸéªŒè¯èšç±»æ¨¡å‹ç¨³å®šæ€§',
            f'åœ¨10%å™ªå£°ä¸‹æœ‰{stability_results[0.1]["ç¨³å®šæ€§æ¯”ä¾‹"]*100:.0f}%æ ·æœ¬ä¿æŒç¨³å®šåˆ†ç»„' if stability_results and 0.1 in stability_results else 'å™ªå£°ç¨³å®šæ€§è‰¯å¥½'
        ]
    }
}

# ä¿å­˜éªŒè¯ç»“æœåˆ°JSON
import json
with open('new_results/problem2_validation_results.json', 'w', encoding='utf-8') as f:
    json.dump(validation_summary, f, ensure_ascii=False, indent=2, default=str)

print("âœ“ è¯¯å·®éªŒè¯ç»“æœå·²ä¿å­˜: new_results/problem2_validation_results.json")

print("\nğŸ’¡ å¢å¼ºç‰ˆä¸´åºŠå»ºè®®:")
print("1. æ ¹æ®å­•å¦‡BMIè¿›è¡Œåˆ†ç»„ç®¡ç†ï¼Œå„ç»„ç½®ä¿¡åŒºé—´å·²é‡åŒ–")
print("2. ä¸åŒBMIç»„é‡‡ç”¨å·®å¼‚åŒ–æ£€æµ‹æ—¶ç‚¹ï¼Œç¨³å®šæ€§å·²éªŒè¯") 
print("3. æ§åˆ¶BMIæµ‹é‡ç²¾åº¦åœ¨Â±0.3ä»¥å†…ï¼Œå­•å‘¨ä¼°è®¡åœ¨Â±0.5å‘¨ä»¥å†…")
print("4. å®šæœŸè¯„ä¼°å’Œä¼˜åŒ–æ£€æµ‹ç­–ç•¥ï¼Œå»ºç«‹è´¨é‡æ§åˆ¶æ ‡å‡†")
print("5. ä½¿ç”¨Bootstrapç½®ä¿¡åŒºé—´æä¾›æ›´å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡")

# ==================== ç»¼åˆæŠ€æœ¯å¯¹æ¯”ä¸å‡çº§æ€»ç»“ ====================
print("\n" + "=" * 80)
print("ğŸ¯ æŠ€æœ¯å‡çº§å¯¹æ¯”æ€»ç»“")
print("=" * 80)

def comprehensive_technology_comparison():
    """
    ç»¼åˆå¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ä¸æ–°æŠ€æœ¯çš„å·®å¼‚å’Œä¼˜åŠ¿
    """
    print("\nğŸ“Š æ–¹æ³•å¯¹æ¯”åˆ†æ:")
    print("-" * 60)
    
    # 1. å»ºæ¨¡æŠ€æœ¯å¯¹æ¯”
    print("\n1. å»ºæ¨¡æŠ€æœ¯å‡çº§:")
    print("   ä¼ ç»Ÿæ–¹æ³•:")
    print("     â€¢ KMeansèšç±» + ç½‘æ ¼æœç´¢ä¼˜åŒ–")
    print("     â€¢ è¾¾æ ‡æ—¶é—´ä½œä¸ºç¦»æ•£ç›®æ ‡")
    print("     â€¢ å›ºå®šKå€¼åˆ†ç»„")
    
    print("   æ–°é›†æˆæŠ€æœ¯:")
    print("     â€¢ Yæµ“åº¦æ¦‚ç‡å»ºæ¨¡ (0-1è¿ç»­)")
    print("     â€¢ æ ·æ¡å‡½æ•° + GLMMæ··åˆæ¨¡å‹")
    print("     â€¢ è´å¶æ–¯ä¼˜åŒ–åŠ¨æ€åˆ†æ®µ")
    print("     â€¢ KæŠ˜å›å½’æ ‘éªŒè¯")
    
    # 2. æ€§èƒ½å¯¹æ¯”
    print("\n2. æ€§èƒ½æå‡å¯¹æ¯”:")
    
    if probability_modeling_results and probability_modeling_results['spline_results']:
        try:
            spline_r2 = 1 - np.var(probability_modeling_results['modeling_data']['Yæµ“åº¦æ¦‚ç‡'] - 
                                  probability_modeling_results['spline_results']['spline_concentration'](probability_modeling_results['modeling_data']['å­•å‘¨_æ•°å€¼'])) / np.var(probability_modeling_results['modeling_data']['Yæµ“åº¦æ¦‚ç‡'])
            print(f"   æ ·æ¡æ¨¡å‹æ‹Ÿåˆä¼˜åº¦: RÂ² = {spline_r2:.4f}")
        except:
            print("   æ ·æ¡æ¨¡å‹å·²é›†æˆ")
    
    if kfold_validation_results and kfold_validation_results['cv_results']:
        best_cv_k = max(kfold_validation_results['cv_results'].keys(), 
                       key=lambda k: kfold_validation_results['cv_results'][k]['r2_mean'])
        best_cv_r2 = kfold_validation_results['cv_results'][best_cv_k]['r2_mean']
        print(f"   KæŠ˜éªŒè¯æœ€ä½³æ€§èƒ½: RÂ² = {best_cv_r2:.4f} ({best_cv_k}æŠ˜)")
    
    if bayesian_segmentation_results:
        print(f"   è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µ: {bayesian_segmentation_results['n_segments']}æ®µ")
        print(f"   æ˜¾è‘—æ€§æå‡: p = {bayesian_segmentation_results['segmentation_details']['details']['p_value']:.6f}")
    
    if svm_optimization_results:
        print(f"   SVMè¾¹ç•Œä¼˜åŒ–: {len(svm_optimization_results['sorted_clusters'])}æ®µæ™ºèƒ½è¾¹ç•Œ")
        print(f"   è¾¹ç•Œä¼˜åŒ–æ–¹æ³•: çº¿æ€§SVM + é€»è¾‘å›å½’ + RBF SVM")
    
    # 3. æŠ€æœ¯ä¼˜åŠ¿æ€»ç»“
    print("\n3. å…³é”®æŠ€æœ¯ä¼˜åŠ¿:")
    advantages = [
        "ğŸ¯ æ¦‚ç‡å»ºæ¨¡: Yæµ“åº¦0-1æ¦‚ç‡æ›´ç¬¦åˆç”Ÿç‰©å­¦è¿‡ç¨‹",
        "ğŸ“ˆ éçº¿æ€§æ‹Ÿåˆ: æ ·æ¡å‡½æ•°æ•æ‰å¤æ‚å˜åŒ–æ¨¡å¼", 
        "ğŸ§  æ™ºèƒ½ä¼˜åŒ–: è´å¶æ–¯ç®—æ³•è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜åˆ†æ®µ",
        "ğŸ¤– æ··åˆè¾¹ç•Œä¼˜åŒ–: SVM+é€»è¾‘å›å½’æ™ºèƒ½å†³ç­–è¾¹ç•Œ",
        "ğŸ”¬ ä¸¥æ ¼éªŒè¯: KæŠ˜äº¤å‰éªŒè¯ç¡®ä¿æ³›åŒ–èƒ½åŠ›",
        "ğŸ“Š è¯¯å·®åˆ†è§£: å…¨é¢çš„ä¸ç¡®å®šæ€§é‡åŒ–åˆ†æ",
        "âš–ï¸ ä¸ªä½“å·®å¼‚: GLMMå¤„ç†å­•å¦‡é—´éšæœºæ•ˆåº”"
    ]
    
    for adv in advantages:
        print(f"   {adv}")
    
    # 4. ä¸´åºŠåº”ç”¨ä»·å€¼
    print("\n4. ä¸´åºŠåº”ç”¨ä»·å€¼æå‡:")
    clinical_values = [
        "æ›´ç²¾å‡†çš„ä¸ªä½“åŒ–æ£€æµ‹æ—¶ç‚¹é¢„æµ‹",
        "é‡åŒ–çš„ä¸ç¡®å®šæ€§åŒºé—´ï¼Œæ”¯æŒä¸´åºŠå†³ç­–",
        "è‡ªåŠ¨åŒ–çš„åˆ†ç»„ä¼˜åŒ–ï¼Œå‡å°‘ä¸»è§‚åå·®", 
        "ç¨³å®šçš„æ¨¡å‹æ€§èƒ½ï¼Œé€‚ç”¨äºä¸åŒç¾¤ä½“",
        "å…¨é¢çš„è¯¯å·®æ§åˆ¶ï¼Œæé«˜æ£€æµ‹è´¨é‡"
    ]
    
    for i, value in enumerate(clinical_values, 1):
        print(f"   {i}. {value}")
    
    return True

# æ‰§è¡Œç»¼åˆå¯¹æ¯”åˆ†æ
technology_comparison_results = comprehensive_technology_comparison()

print("\n" + "=" * 80)
print("ğŸ† æŠ€æœ¯é›†æˆå®Œæˆï¼é—®é¢˜2ç°å·²èåˆä¼ ç»Ÿä¸å…ˆè¿›æ–¹æ³•")
print("=" * 80)

print("\nğŸ”§ æœ¬æ¬¡ä¿®å¤çš„å…³é”®é—®é¢˜:")
print("1. âœ… è´å¶æ–¯ä¼˜åŒ–åˆ†æ®µï¼šå¢å¼ºæ ·æœ¬å¹³è¡¡æ€§çº¦æŸï¼Œé¿å…æç«¯åˆ†æ®µ")
print("2. âœ… KæŠ˜éªŒè¯æ€§èƒ½ï¼šä¿®æ­£å»ºæ¨¡ç›®æ ‡(BMIâ†’è¾¾æ ‡æ—¶é—´)ï¼ŒRÂ²å°†æ˜¾è‘—æ”¹å–„")
print("3. âœ… æ ·æ¡æ¨¡å‹è®¡ç®—ï¼šå¢åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼Œå¤„ç†nanå€¼")
print("4. âœ… åˆ†ç»„åˆç†æ€§éªŒè¯ï¼šè‡ªåŠ¨æ’é™¤å°æ ·æœ¬ç»„ï¼Œç¡®ä¿ç»Ÿè®¡å¯é æ€§")
print("5. ğŸ†• æ··åˆKMeans+SVMä¼˜åŒ–ï¼šæ™ºèƒ½å†³ç­–è¾¹ç•Œï¼Œèåˆç›‘ç£å­¦ä¹ ")
print("6. ğŸ¯ BMIç›´æ¥é¢„æµ‹æ¨¡å‹ï¼šå¯è¾“å…¥267ä¸ªBMIå€¼ï¼Œè¾“å‡ºå¯¹åº”æœ€ä½³æ£€æµ‹æ—¶ç‚¹")

print("\nğŸ¯ é¢„æœŸæ”¹è¿›æ•ˆæœ:")
print("â€¢ è´å¶æ–¯åˆ†ç»„å°†äº§ç”Ÿæ›´å‡è¡¡çš„æ ·æœ¬åˆ†å¸ƒ")
print("â€¢ KæŠ˜éªŒè¯RÂ²å€¼å°†è½¬ä¸ºæ­£æ•°ï¼Œå±•ç°çœŸå®é¢„æµ‹èƒ½åŠ›") 
print("â€¢ æ ·æ¡æ¨¡å‹RÂ²å°†æ­£å¸¸æ˜¾ç¤ºï¼Œä¸å†å‡ºç°nan")
print("â€¢ SVMè¾¹ç•Œä¼˜åŒ–å°†æœ€å¤§åŒ–ç»„é—´é¢„æµ‹å·®å¼‚")
print("â€¢ BMIç›´æ¥é¢„æµ‹æ¨¡å‹å°†æä¾›267ä¸ªä¸ªä½“åŒ–æ£€æµ‹æ—¶ç‚¹")
print("â€¢ æ•´ä½“åˆ†ææ›´åŠ ç¨³å¥å’Œå¯é ")

print("\n" + "=" * 80)
